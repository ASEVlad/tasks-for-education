{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_task_nlp_hard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9LtSCnygLEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3XwP4LygM22",
        "colab_type": "code",
        "outputId": "2af14e65-a1e9-4c40-bb3b-87be330fea33",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "files.upload()\n",
        "commits = pd.read_csv('commits.csv')\n",
        "issues = pd.read_csv('issues.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee2086f3-5fb8-408c-822d-413d0ab4052a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ee2086f3-5fb8-408c-822d-413d0ab4052a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving commits.csv to commits.csv\n",
            "Saving issues.csv to issues.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFatfKAQga_v",
        "colab_type": "code",
        "outputId": "a6f5fc9f-737c-498b-944d-0cc1e6fc2d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "df = pd.concat([commits, issues], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>cid</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>time</th>\n",
              "      <th>time_offset</th>\n",
              "      <th>tree_id</th>\n",
              "      <th>message_encoding</th>\n",
              "      <th>key</th>\n",
              "      <th>jira_id</th>\n",
              "      <th>status_category</th>\n",
              "      <th>creator</th>\n",
              "      <th>priority</th>\n",
              "      <th>status</th>\n",
              "      <th>assignee</th>\n",
              "      <th>issuetype</th>\n",
              "      <th>reporter</th>\n",
              "      <th>resolution</th>\n",
              "      <th>project</th>\n",
              "      <th>updated</th>\n",
              "      <th>created</th>\n",
              "      <th>resolved</th>\n",
              "      <th>components</th>\n",
              "      <th>severity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SPARK-16904</td>\n",
              "      <td>57626a55703a189e03148398f67c36cd0e557044</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478510197</td>\n",
              "      <td>-480</td>\n",
              "      <td>0f4ae99e29245041cae4d41f58aa208dba6beea4</td>\n",
              "      <td>[SPARK-16904][SQL] Removal of Hive Built-in Ha...</td>\n",
              "      <td>SPARK-4957</td>\n",
              "      <td>12763687.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>nathan_gs</td>\n",
              "      <td>Major</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>nathan_gs</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-06T01:43:57.000+0000</td>\n",
              "      <td>2014-12-24T13:38:57.000+0000</td>\n",
              "      <td>2016-09-06T01:43:56.000+0000</td>\n",
              "      <td>Scheduler</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPARK-18296</td>\n",
              "      <td>9db06c442cf85e41d51c7b167817f4e7971bf0da</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478501095</td>\n",
              "      <td>-480</td>\n",
              "      <td>e6325a24125325611506a7fc0119209b88be17e3</td>\n",
              "      <td>[SPARK-18296][SQL] Use consistent naming for e...</td>\n",
              "      <td>SPARK-4956</td>\n",
              "      <td>12763679.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>liaoyuxi</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>liaoyuxi</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-04T05:16:07.000+0000</td>\n",
              "      <td>2014-12-24T12:27:12.000+0000</td>\n",
              "      <td>2015-02-23T22:34:41.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPARK-18167</td>\n",
              "      <td>07ac3f09daf2b28436bc69f76badd1e36d756e4d</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478500925</td>\n",
              "      <td>-480</td>\n",
              "      <td>525bd0fd99741a6a0e2bcfdd5c4e074f839b92a5</td>\n",
              "      <td>[SPARK-18167][SQL] Disable flaky hive partitio...</td>\n",
              "      <td>SPARK-4955</td>\n",
              "      <td>12763661.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chengxiang li</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>lianhuiwang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chengxiang li</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-28T20:54:23.000+0000</td>\n",
              "      <td>2014-12-24T09:16:23.000+0000</td>\n",
              "      <td>2015-01-28T20:54:23.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SPARK-18173</td>\n",
              "      <td>46b2e499935386e28899d860110a6ab16c107c0c</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478487433</td>\n",
              "      <td>-480</td>\n",
              "      <td>a27367a0be38e5a8814aac6295387f0457c603c8</td>\n",
              "      <td>[SPARK-18173][SQL] data source tables should s...</td>\n",
              "      <td>SPARK-4954</td>\n",
              "      <td>12763657.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>liyezhang556520</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>liyezhang556520</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>liyezhang556520</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-27T07:25:26.000+0000</td>\n",
              "      <td>2014-12-24T09:01:34.000+0000</td>\n",
              "      <td>2014-12-27T07:25:26.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SPARK-18269</td>\n",
              "      <td>556a3b7d07f36c29ceb88fb6c24cc229e0e53ee4</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478487125</td>\n",
              "      <td>-480</td>\n",
              "      <td>ba48a84dc34757cd6691b6a7ce9abdb615c4c1ac</td>\n",
              "      <td>[SPARK-18269][SQL] CSV datasource should read ...</td>\n",
              "      <td>SPARK-4953</td>\n",
              "      <td>12763643.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-25T15:06:12.000+0000</td>\n",
              "      <td>2014-12-24T07:07:56.000+0000</td>\n",
              "      <td>2014-12-25T15:06:12.000+0000</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           key  ... severity\n",
              "0  SPARK-16904  ...      3.0\n",
              "1  SPARK-18296  ...      1.0\n",
              "2  SPARK-18167  ...     10.0\n",
              "3  SPARK-18173  ...      1.0\n",
              "4  SPARK-18269  ...      3.0\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NyqyfV9gh4c",
        "colab_type": "code",
        "outputId": "c397b480-fece-4778-8196-7ec902373f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "df['author_name'].value_counts()[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Matei Zaharia      1599\n",
              "Reynold Xin        1459\n",
              "Patrick Wendell     865\n",
              "Josh Rosen          564\n",
              "Davies Liu          506\n",
              "Tathagata Das       501\n",
              "Andrew Or           460\n",
              "Cheng Lian          391\n",
              "Xiangrui Meng       372\n",
              "Wenchen Fan         365\n",
              "Name: author_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go923TorhLwm",
        "colab_type": "code",
        "outputId": "e2bccae9-71e3-46cd-a3d8-1ceadaabbf05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df[df['issuetype'] == 'Bug']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>cid</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>time</th>\n",
              "      <th>time_offset</th>\n",
              "      <th>tree_id</th>\n",
              "      <th>message_encoding</th>\n",
              "      <th>key</th>\n",
              "      <th>jira_id</th>\n",
              "      <th>status_category</th>\n",
              "      <th>creator</th>\n",
              "      <th>priority</th>\n",
              "      <th>status</th>\n",
              "      <th>assignee</th>\n",
              "      <th>issuetype</th>\n",
              "      <th>reporter</th>\n",
              "      <th>resolution</th>\n",
              "      <th>project</th>\n",
              "      <th>updated</th>\n",
              "      <th>created</th>\n",
              "      <th>resolved</th>\n",
              "      <th>components</th>\n",
              "      <th>severity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPARK-18296</td>\n",
              "      <td>9db06c442cf85e41d51c7b167817f4e7971bf0da</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478501095</td>\n",
              "      <td>-480</td>\n",
              "      <td>e6325a24125325611506a7fc0119209b88be17e3</td>\n",
              "      <td>[SPARK-18296][SQL] Use consistent naming for e...</td>\n",
              "      <td>SPARK-4956</td>\n",
              "      <td>12763679.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>liaoyuxi</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>liaoyuxi</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-04T05:16:07.000+0000</td>\n",
              "      <td>2014-12-24T12:27:12.000+0000</td>\n",
              "      <td>2015-02-23T22:34:41.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPARK-18167</td>\n",
              "      <td>07ac3f09daf2b28436bc69f76badd1e36d756e4d</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478500925</td>\n",
              "      <td>-480</td>\n",
              "      <td>525bd0fd99741a6a0e2bcfdd5c4e074f839b92a5</td>\n",
              "      <td>[SPARK-18167][SQL] Disable flaky hive partitio...</td>\n",
              "      <td>SPARK-4955</td>\n",
              "      <td>12763661.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chengxiang li</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>lianhuiwang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chengxiang li</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-28T20:54:23.000+0000</td>\n",
              "      <td>2014-12-24T09:16:23.000+0000</td>\n",
              "      <td>2015-01-28T20:54:23.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SPARK-18269</td>\n",
              "      <td>556a3b7d07f36c29ceb88fb6c24cc229e0e53ee4</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478487125</td>\n",
              "      <td>-480</td>\n",
              "      <td>ba48a84dc34757cd6691b6a7ce9abdb615c4c1ac</td>\n",
              "      <td>[SPARK-18269][SQL] CSV datasource should read ...</td>\n",
              "      <td>SPARK-4953</td>\n",
              "      <td>12763643.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-25T15:06:12.000+0000</td>\n",
              "      <td>2014-12-24T07:07:56.000+0000</td>\n",
              "      <td>2014-12-25T15:06:12.000+0000</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SPARK-18210</td>\n",
              "      <td>b89d0556dff0520ab35882382242fbfa7d9478eb</td>\n",
              "      <td>Wojciech Szymanski</td>\n",
              "      <td>wk.szymanski@gmail.com</td>\n",
              "      <td>Yanbo Liang</td>\n",
              "      <td>ybliang8@gmail.com</td>\n",
              "      <td>1478446993</td>\n",
              "      <td>-480</td>\n",
              "      <td>0ecb3c3e4b54333c254cfcdd3af6a0427c6e850f</td>\n",
              "      <td>[SPARK-18210][ML] Pipeline.copy does not creat...</td>\n",
              "      <td>SPARK-4952</td>\n",
              "      <td>12763642.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>gq</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>gq</td>\n",
              "      <td>Bug</td>\n",
              "      <td>gq</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-08T14:17:37.000+0000</td>\n",
              "      <td>2014-12-24T06:42:43.000+0000</td>\n",
              "      <td>2014-12-27T07:32:38.000+0000</td>\n",
              "      <td>Spark Core,YARN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SPARK-17854</td>\n",
              "      <td>340f09d100cb669bc6795f085aac6fa05630a076</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1478441497</td>\n",
              "      <td>0</td>\n",
              "      <td>9c55ecdcea33432ed7ca9a76ca0be33d6be26268</td>\n",
              "      <td>[SPARK-17854][SQL] rand/randn allows null/long...</td>\n",
              "      <td>SPARK-4951</td>\n",
              "      <td>12763639.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>zsxwing</td>\n",
              "      <td>Major</td>\n",
              "      <td>Closed</td>\n",
              "      <td>zsxwing</td>\n",
              "      <td>Bug</td>\n",
              "      <td>zsxwing</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-12T00:24:24.000+0000</td>\n",
              "      <td>2014-12-24T05:29:48.000+0000</td>\n",
              "      <td>2015-01-12T00:24:24.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>15d392688456ad9f963417843c52a7b610f771d2</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>Felix Cheung</td>\n",
              "      <td>felixcheung@apache.org</td>\n",
              "      <td>1478407653</td>\n",
              "      <td>-420</td>\n",
              "      <td>4a969525408e41c1bc971c9b79ab20ad741a3962</td>\n",
              "      <td>[MINOR][DOCUMENTATION] Fix some minor descript...</td>\n",
              "      <td>SPARK-4949</td>\n",
              "      <td>12763627.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-18T12:20:23.000+0000</td>\n",
              "      <td>2014-12-24T04:32:00.000+0000</td>\n",
              "      <td>2015-02-18T12:20:23.000+0000</td>\n",
              "      <td>Scheduler</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SPARK-18101</td>\n",
              "      <td>95ec4e25bb65f37f80222ffe70a95993a9149f80</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478332730</td>\n",
              "      <td>-420</td>\n",
              "      <td>0e9455a041626ffa56f2c83d9cdad02843a72a83</td>\n",
              "      <td>[SPARK-17183][SPARK-17983][SPARK-18101][SQL] p...</td>\n",
              "      <td>SPARK-4944</td>\n",
              "      <td>12763604.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-15T22:09:40.000+0000</td>\n",
              "      <td>2014-12-24T01:35:02.000+0000</td>\n",
              "      <td>2016-06-15T22:08:44.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SPARK-17183</td>\n",
              "      <td>95ec4e25bb65f37f80222ffe70a95993a9149f80</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478332730</td>\n",
              "      <td>-420</td>\n",
              "      <td>0e9455a041626ffa56f2c83d9cdad02843a72a83</td>\n",
              "      <td>[SPARK-17183][SPARK-17983][SPARK-18101][SQL] p...</td>\n",
              "      <td>SPARK-4943</td>\n",
              "      <td>12763602.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>alexliu68</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>alexliu68</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-15T14:17:46.000+0000</td>\n",
              "      <td>2014-12-24T01:19:51.000+0000</td>\n",
              "      <td>2015-01-10T21:42:56.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SPARK-18260</td>\n",
              "      <td>6e2701815761d5870111cb56300e30d3059b39ed</td>\n",
              "      <td>Burak Yavuz</td>\n",
              "      <td>brkyvz@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478329671</td>\n",
              "      <td>-420</td>\n",
              "      <td>4c19428fe519db9f4ab5a957b5b7f8174e2932cd</td>\n",
              "      <td>[SPARK-18260] Make from_json null safe\\n\\n## W...</td>\n",
              "      <td>SPARK-4941</td>\n",
              "      <td>12763533.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>gst</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>gst</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-28T12:35:08.000+0000</td>\n",
              "      <td>2014-12-23T19:43:58.000+0000</td>\n",
              "      <td>2015-03-28T12:35:08.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SPARK-18189</td>\n",
              "      <td>0f7c9e84e0d00813bf56712097677add5657f19f</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478327669</td>\n",
              "      <td>-420</td>\n",
              "      <td>29ca94f215b1d0c9a6201309f180c578a719edbd</td>\n",
              "      <td>[SPARK-18189] [SQL] [Followup] Move test from ...</td>\n",
              "      <td>SPARK-4939</td>\n",
              "      <td>12763508.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>davies</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>davies</td>\n",
              "      <td>Bug</td>\n",
              "      <td>davies</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-05-21T23:34:03.000+0000</td>\n",
              "      <td>2014-12-23T18:19:47.000+0000</td>\n",
              "      <td>2015-02-04T22:25:01.000+0000</td>\n",
              "      <td>DStreams,PySpark,Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SPARK-18197</td>\n",
              "      <td>a42d738c5de08bd395a7c220c487146173c6c163</td>\n",
              "      <td>Adam Roberts</td>\n",
              "      <td>aroberts@uk.ibm.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478286366</td>\n",
              "      <td>-420</td>\n",
              "      <td>1018e34a21f2145ad8a42ea6f58a876ce678dc2c</td>\n",
              "      <td>[SPARK-18197][CORE] Optimise AppendOnlyMap imp...</td>\n",
              "      <td>SPARK-4935</td>\n",
              "      <td>12763427.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>scwf</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>scwf</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-30T21:44:37.000+0000</td>\n",
              "      <td>2014-12-23T09:53:39.000+0000</td>\n",
              "      <td>2014-12-30T21:44:37.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SPARK-18200</td>\n",
              "      <td>27602c33751cebf6cd173c0de103454608cf6625</td>\n",
              "      <td>Dongjoon Hyun</td>\n",
              "      <td>dongjoon@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478240133</td>\n",
              "      <td>-420</td>\n",
              "      <td>8b058a154a68bdd9c8d178730ba18aec68b10fd2</td>\n",
              "      <td>[SPARK-18200][GRAPHX][FOLLOW-UP] Support zero ...</td>\n",
              "      <td>SPARK-4933</td>\n",
              "      <td>12763411.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>liyezhang556520</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>liyezhang556520</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-23T22:35:52.000+0000</td>\n",
              "      <td>2014-12-23T08:22:06.000+0000</td>\n",
              "      <td>2014-12-23T22:35:52.000+0000</td>\n",
              "      <td>Spark Core,Web UI</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SPARK-18257</td>\n",
              "      <td>f22954ad49bf5a32c7b6d8487cd38ffe0da904ca</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478212245</td>\n",
              "      <td>-420</td>\n",
              "      <td>7c9ca168516da1cc9acd4ba8bde1d9302b439e1b</td>\n",
              "      <td>[SPARK-18257][SS] Improve error reporting for ...</td>\n",
              "      <td>SPARK-4929</td>\n",
              "      <td>12763373.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>carlmartin</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>carlmartin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-07T14:11:48.000+0000</td>\n",
              "      <td>2014-12-23T04:15:25.000+0000</td>\n",
              "      <td>2015-01-07T14:11:48.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>e89202523bc2f5573bd736278c6b96c6e6759909</td>\n",
              "      <td>wm624@hotmail.com</td>\n",
              "      <td>wm624@hotmail.com</td>\n",
              "      <td>Felix Cheung</td>\n",
              "      <td>felixcheung@apache.org</td>\n",
              "      <td>1478212038</td>\n",
              "      <td>-420</td>\n",
              "      <td>a3684731d25172d2f65001dc2018ca9d910fbee9</td>\n",
              "      <td>[SPARKR][TEST] remove unnecessary suppressWarn...</td>\n",
              "      <td>SPARK-4928</td>\n",
              "      <td>12763361.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>guowei2</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>guowei2</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-30T20:37:43.000+0000</td>\n",
              "      <td>2014-12-23T02:33:46.000+0000</td>\n",
              "      <td>2014-12-30T20:37:43.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>SPARK-18212</td>\n",
              "      <td>67659c9afaeb2289e56fd87fafee953e8f050383</td>\n",
              "      <td>cody koeninger</td>\n",
              "      <td>cody@koeninger.org</td>\n",
              "      <td>Michael Armbrust</td>\n",
              "      <td>michael@databricks.com</td>\n",
              "      <td>1478209405</td>\n",
              "      <td>-420</td>\n",
              "      <td>6a00e1c2e005a4d1d099e72b1c0fa665f6872328</td>\n",
              "      <td>[SPARK-18212][SS][KAFKA] increase executor pol...</td>\n",
              "      <td>SPARK-4927</td>\n",
              "      <td>12763359.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>ilganeli</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>ilganeli</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-04-01T10:57:18.000+0000</td>\n",
              "      <td>2014-12-23T02:24:15.000+0000</td>\n",
              "      <td>2015-04-01T10:57:18.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>SPARK-18244</td>\n",
              "      <td>b17057c0a69b9c56e503483d97f5dc209eef0884</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478198885</td>\n",
              "      <td>-420</td>\n",
              "      <td>dc36d1f309c2deca27d585c6e4b57e5f2989515a</td>\n",
              "      <td>[SPARK-18244][SQL] Rename partitionProviderIsH...</td>\n",
              "      <td>SPARK-4923</td>\n",
              "      <td>12763321.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>peng</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>senkwich</td>\n",
              "      <td>Bug</td>\n",
              "      <td>peng</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-29T08:13:51.000+0000</td>\n",
              "      <td>2014-12-22T22:07:39.000+0000</td>\n",
              "      <td>2015-01-20T21:36:26.000+0000</td>\n",
              "      <td>Build,Spark Shell</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>SPARK-17949</td>\n",
              "      <td>27daf6bcde782ed3e0f0d951c90c8040fd47e985</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>Yin Huai</td>\n",
              "      <td>yhuai@databricks.com</td>\n",
              "      <td>1478190891</td>\n",
              "      <td>-420</td>\n",
              "      <td>d56b006e8f954af73ee46d5084e1c5b855334cfb</td>\n",
              "      <td>[SPARK-17949][SQL] A JVM object based aggregat...</td>\n",
              "      <td>SPARK-4922</td>\n",
              "      <td>12763289.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-07-20T08:15:03.000+0000</td>\n",
              "      <td>2014-12-22T18:57:31.000+0000</td>\n",
              "      <td>2015-07-20T08:15:03.000+0000</td>\n",
              "      <td>Mesos</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SPARK-17957</td>\n",
              "      <td>66a99f4a411ee7dc94ff1070a8fd6865fd004093</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>Herman van Hovell</td>\n",
              "      <td>hvanhovell@databricks.com</td>\n",
              "      <td>1478187336</td>\n",
              "      <td>60</td>\n",
              "      <td>9448e766cb3719a1c12b33a2158a12aa8589e9a6</td>\n",
              "      <td>[SPARK-17981][SPARK-17957][SQL] Fix Incorrect ...</td>\n",
              "      <td>SPARK-4921</td>\n",
              "      <td>12763286.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>xuefuz</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>xuefuz</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-10T18:15:42.000+0000</td>\n",
              "      <td>2014-12-22T18:35:10.000+0000</td>\n",
              "      <td>2015-03-10T18:15:42.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>SPARK-18175</td>\n",
              "      <td>9ddec8636c4f5e8c4592aefecec9886b409ced8f</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>1478145663</td>\n",
              "      <td>-420</td>\n",
              "      <td>a8c745557d487e7095933196ef47f4d723e075f0</td>\n",
              "      <td>[SPARK-18175][SQL] Improve the test case cover...</td>\n",
              "      <td>SPARK-4914</td>\n",
              "      <td>12763130.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Bug</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-23T20:54:59.000+0000</td>\n",
              "      <td>2014-12-21T16:24:03.000+0000</td>\n",
              "      <td>2014-12-23T20:54:47.000+0000</td>\n",
              "      <td>Build</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>SPARK-17963</td>\n",
              "      <td>7eb2ca8e338e04034a662920261e028f56b07395</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>1478145390</td>\n",
              "      <td>-420</td>\n",
              "      <td>60ba82749182efb7bc86408985dba150bf4e1b99</td>\n",
              "      <td>[SPARK-17963][SQL][DOCUMENTATION] Add examples...</td>\n",
              "      <td>SPARK-4913</td>\n",
              "      <td>12763129.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>viirya</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>viirya</td>\n",
              "      <td>Bug</td>\n",
              "      <td>viirya</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-23T22:59:46.000+0000</td>\n",
              "      <td>2014-12-21T16:14:44.000+0000</td>\n",
              "      <td>2014-12-23T22:59:35.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>SPARK-18214</td>\n",
              "      <td>fd90541c35af2bccf0155467bec8cea7c8865046</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478127182</td>\n",
              "      <td>-420</td>\n",
              "      <td>628164701b38ddebfd0e916a53bffba01c716a00</td>\n",
              "      <td>[SPARK-18214][SQL] Simplify RuntimeReplaceable...</td>\n",
              "      <td>SPARK-4910</td>\n",
              "      <td>12763071.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>xhudik</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>xhudik</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-21T21:20:33.000+0000</td>\n",
              "      <td>2014-12-20T14:08:27.000+0000</td>\n",
              "      <td>2014-12-21T21:20:33.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>SPARK-17058</td>\n",
              "      <td>37d95227a21de602b939dae84943ba007f434513</td>\n",
              "      <td>Steve Loughran</td>\n",
              "      <td>stevel@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478112749</td>\n",
              "      <td>-420</td>\n",
              "      <td>d55490b0d67566c7d178064ad386d74be8aa22be</td>\n",
              "      <td>[SPARK-17058][BUILD] Add maven snapshots-and-s...</td>\n",
              "      <td>SPARK-4909</td>\n",
              "      <td>12763060.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>shenhong</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>shenhong</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-03-24T21:19:06.000+0000</td>\n",
              "      <td>2014-12-20T09:00:32.000+0000</td>\n",
              "      <td>2014-12-25T15:20:54.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>SPARK-18160</td>\n",
              "      <td>3c24299b71e23e159edbb972347b13430f92a465</td>\n",
              "      <td>Jeff Zhang</td>\n",
              "      <td>zjffdu@apache.org</td>\n",
              "      <td>Marcelo Vanzin</td>\n",
              "      <td>vanzin@cloudera.com</td>\n",
              "      <td>1478112465</td>\n",
              "      <td>-420</td>\n",
              "      <td>af6c8aa0fe13aaf1b35fa424667f04ca1e217cf0</td>\n",
              "      <td>[SPARK-18160][CORE][YARN] spark.files &amp; spark....</td>\n",
              "      <td>SPARK-4908</td>\n",
              "      <td>12762999.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>dyross</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Bug</td>\n",
              "      <td>dyross</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-12T07:11:26.000+0000</td>\n",
              "      <td>2014-12-19T21:45:25.000+0000</td>\n",
              "      <td>2014-12-30T19:25:21.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>SPARK-14393</td>\n",
              "      <td>02f203107b8eda1f1576e36c4f12b0e3bc5e910e</td>\n",
              "      <td>Xiangrui Meng</td>\n",
              "      <td>meng@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478112109</td>\n",
              "      <td>-420</td>\n",
              "      <td>46bba2bdb7045caa74ecd9dd3a6a05b3972e69f2</td>\n",
              "      <td>[SPARK-14393][SQL] values generated by non-det...</td>\n",
              "      <td>SPARK-4907</td>\n",
              "      <td>12762988.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Bug</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-26T07:43:27.000+0000</td>\n",
              "      <td>2014-12-19T20:27:23.000+0000</td>\n",
              "      <td>2014-12-23T00:43:12.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>SPARK-17895</td>\n",
              "      <td>742e0fea5391857964e90d396641ecf95cac4248</td>\n",
              "      <td>buzhihuojie</td>\n",
              "      <td>ren.weiluo@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478111780</td>\n",
              "      <td>-420</td>\n",
              "      <td>3fbd75cfa136e7ef9991f9431d88b99183a791f4</td>\n",
              "      <td>[SPARK-17895] Improve doc for rangeBetween and...</td>\n",
              "      <td>SPARK-4906</td>\n",
              "      <td>12762931.0</td>\n",
              "      <td>New</td>\n",
              "      <td>mkim</td>\n",
              "      <td>Major</td>\n",
              "      <td>Open</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>mkim</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-26T14:49:08.000+0000</td>\n",
              "      <td>2014-12-19T18:44:52.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Web UI</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>SPARK-17683</td>\n",
              "      <td>4af0ce2d96de3397c9bc05684cad290a52486577</td>\n",
              "      <td>Takeshi YAMAMURO</td>\n",
              "      <td>linguin.m.s@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1478111366</td>\n",
              "      <td>-420</td>\n",
              "      <td>719eabf854d638757402f9e3cb1aac68f2a756e6</td>\n",
              "      <td>[SPARK-17683][SQL] Support ArrayType in Litera...</td>\n",
              "      <td>SPARK-4905</td>\n",
              "      <td>12762881.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>hshreedharan</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-19T02:48:19.000+0000</td>\n",
              "      <td>2014-12-19T16:20:14.000+0000</td>\n",
              "      <td>2015-02-09T22:17:47.000+0000</td>\n",
              "      <td>DStreams</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>SPARK-18076</td>\n",
              "      <td>9c8deef64efee20a0ddc9b612f90e77c80aede60</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1478079555</td>\n",
              "      <td>0</td>\n",
              "      <td>feb6a7eeb4e10f628ff1227787c1ee430cb6195b</td>\n",
              "      <td>[SPARK-18076][CORE][SQL] Fix default Locale us...</td>\n",
              "      <td>SPARK-4903</td>\n",
              "      <td>12762847.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>evertlammerts</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>evertlammerts</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-18T05:26:31.000+0000</td>\n",
              "      <td>2014-12-19T13:44:00.000+0000</td>\n",
              "      <td>2015-02-13T17:49:22.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>SPARK-18198</td>\n",
              "      <td>98ede49496d0d7b4724085083d4f24436b92a7bf</td>\n",
              "      <td>Liwei Lin</td>\n",
              "      <td>lwlin7@gmail.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1478077834</td>\n",
              "      <td>0</td>\n",
              "      <td>0373c5a17292ed0284ee6d39b800d636c3e292da</td>\n",
              "      <td>[SPARK-18198][DOC][STREAMING] Highlight code s...</td>\n",
              "      <td>SPARK-4901</td>\n",
              "      <td>12762825.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-19T16:05:11.000+0000</td>\n",
              "      <td>2014-12-19T11:34:41.000+0000</td>\n",
              "      <td>2014-12-19T16:05:04.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>NaN</td>\n",
              "      <td>bcbe44440e6c871e217f06d2a4696fd41f1d2606</td>\n",
              "      <td>Maria Rydzy</td>\n",
              "      <td>majrydzy+gh@gmail.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1478077756</td>\n",
              "      <td>0</td>\n",
              "      <td>9cc8b36446b564f704b2bd8f634420a2f7ed2098</td>\n",
              "      <td>[MINOR] Use &lt;= for clarity in Pi examples' Mon...</td>\n",
              "      <td>SPARK-4900</td>\n",
              "      <td>12762822.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>mbofb</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>mbofb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-04-04T15:55:04.000+0000</td>\n",
              "      <td>2014-12-19T11:23:04.000+0000</td>\n",
              "      <td>2015-04-04T15:54:51.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>SPARK-17838</td>\n",
              "      <td>1ecfafa0869cb3a3e367bda8be252a69874dc4de</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>Felix Cheung</td>\n",
              "      <td>felixcheung@apache.org</td>\n",
              "      <td>1478063693</td>\n",
              "      <td>-420</td>\n",
              "      <td>f24f5ae60f65310bde6978590726acfdc16dad1d</td>\n",
              "      <td>[SPARK-17838][SPARKR] Check named arguments fo...</td>\n",
              "      <td>SPARK-4892</td>\n",
              "      <td>12762722.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>pwendell</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>Bug</td>\n",
              "      <td>pwendell</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-05-28T06:07:43.000+0000</td>\n",
              "      <td>2014-12-18T23:52:09.000+0000</td>\n",
              "      <td>2015-05-28T06:07:43.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17905</th>\n",
              "      <td>NaN</td>\n",
              "      <td>79868fe7246d8e6d57e0a376b2593fabea9a9d83</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>1382078360</td>\n",
              "      <td>-420</td>\n",
              "      <td>dbec9cf4160bd1e442714becefa592684d7a61f7</td>\n",
              "      <td>Improved code style.\\n</td>\n",
              "      <td>SPARK-3417</td>\n",
              "      <td>12739500.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>mrocklin</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>mrocklin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-08T22:46:54.000+0000</td>\n",
              "      <td>2014-09-05T15:25:41.000+0000</td>\n",
              "      <td>2014-09-08T22:46:54.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17907</th>\n",
              "      <td>NaN</td>\n",
              "      <td>654d60b6ee65d27545773d3cec9b0ed6ed20ff3a</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>1382074150</td>\n",
              "      <td>-420</td>\n",
              "      <td>213eb51c076d0327bd94564145a15881cb184569</td>\n",
              "      <td>Added dependency on stream-lib version 2.4.0 f...</td>\n",
              "      <td>SPARK-3415</td>\n",
              "      <td>12739470.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>wardviaene</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>wardviaene</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-08T01:54:51.000+0000</td>\n",
              "      <td>2014-09-05T13:10:59.000+0000</td>\n",
              "      <td>2014-09-08T01:54:51.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17908</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ec5df800fdb0109314c0d5cd6dcac2ecbb9433d6</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>1382073960</td>\n",
              "      <td>-420</td>\n",
              "      <td>6bb22cc0e486f3a1effdb4bf0e08008071257e4d</td>\n",
              "      <td>Added countDistinctByKey to PairRDDFunctions t...</td>\n",
              "      <td>SPARK-3414</td>\n",
              "      <td>12739436.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>Bug</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-20T23:41:30.000+0000</td>\n",
              "      <td>2014-09-05T09:13:23.000+0000</td>\n",
              "      <td>2014-09-20T23:41:30.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17909</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1a701358c0811c7f270132291e0646fd806e4984</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>1382073888</td>\n",
              "      <td>-420</td>\n",
              "      <td>599c035a6e05a864420a3e1013a615619705ae57</td>\n",
              "      <td>Added a countDistinct method to RDD that takes...</td>\n",
              "      <td>SPARK-3413</td>\n",
              "      <td>12739341.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>Patrick Liu</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Patrick Liu</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-04-04T16:24:15.000+0000</td>\n",
              "      <td>2014-09-05T08:02:03.000+0000</td>\n",
              "      <td>2015-04-04T16:24:15.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17910</th>\n",
              "      <td>NaN</td>\n",
              "      <td>843727af99786a45cf29352b4e05df92c6b3b6b9</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>Hossein Falaki</td>\n",
              "      <td>falaki@gmail.com</td>\n",
              "      <td>1382073426</td>\n",
              "      <td>-420</td>\n",
              "      <td>5d56f0e6d755014d375ac5f0bc8bc6d06a6782df</td>\n",
              "      <td>Added a serializable wrapper for HyperLogLog\\n</td>\n",
              "      <td>SPARK-3412</td>\n",
              "      <td>12739340.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-10-13T07:47:11.000+0000</td>\n",
              "      <td>2014-09-05T07:53:15.000+0000</td>\n",
              "      <td>2014-10-09T22:01:34.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17914</th>\n",
              "      <td>NaN</td>\n",
              "      <td>9115a5de62dcb832569727773112a4688ef63f03</td>\n",
              "      <td>Binh Nguyen</td>\n",
              "      <td>ngbinh@gmail.com</td>\n",
              "      <td>Binh Nguyen</td>\n",
              "      <td>ngbinh@gmail.com</td>\n",
              "      <td>1387925970</td>\n",
              "      <td>-480</td>\n",
              "      <td>30a8420d87ed51cfeda95e39bd1a03f79fe58190</td>\n",
              "      <td>Remove import * and fix some formatting\\n</td>\n",
              "      <td>SPARK-3408</td>\n",
              "      <td>12739307.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-08T01:43:08.000+0000</td>\n",
              "      <td>2014-09-05T02:18:33.000+0000</td>\n",
              "      <td>2014-09-08T01:43:08.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17916</th>\n",
              "      <td>NaN</td>\n",
              "      <td>55b7e2fdffc6c3537da69152a3d02d5be599fa1b</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>1388513571</td>\n",
              "      <td>-480</td>\n",
              "      <td>6ac21f7d330f020b017534cdc4cfd249426015d5</td>\n",
              "      <td>Merge pull request #289 from tdas/filestream-f...</td>\n",
              "      <td>SPARK-3406</td>\n",
              "      <td>12739290.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>holdenk</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>holdenk</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-06T21:50:16.000+0000</td>\n",
              "      <td>2014-09-05T00:27:08.000+0000</td>\n",
              "      <td>2014-09-06T21:50:16.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17918</th>\n",
              "      <td>NaN</td>\n",
              "      <td>271e3237f3e2efa62a94d6936fce551a40edd65f</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1388176017</td>\n",
              "      <td>-480</td>\n",
              "      <td>9edcf433023191ca80286acbd3e6bd5efd9bebbd</td>\n",
              "      <td>Minor changes in comments and strings to addre...</td>\n",
              "      <td>SPARK-3404</td>\n",
              "      <td>12739168.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>srowen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-09T21:15:01.000+0000</td>\n",
              "      <td>2014-09-04T16:50:14.000+0000</td>\n",
              "      <td>2014-09-09T21:15:01.000+0000</td>\n",
              "      <td>Build</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17919</th>\n",
              "      <td>NaN</td>\n",
              "      <td>3618d70b2a8a66e9a17a7e2efc8c97a22243073a</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1388090740</td>\n",
              "      <td>-480</td>\n",
              "      <td>d2420fd43a0088e9b76db82aa7fa7092101045b0</td>\n",
              "      <td>Added warning if filestream adds files with no...</td>\n",
              "      <td>SPARK-3403</td>\n",
              "      <td>12739151.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>avulanov</td>\n",
              "      <td>Major</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>avulanov</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-23T22:21:10.000+0000</td>\n",
              "      <td>2014-09-04T15:41:00.000+0000</td>\n",
              "      <td>2015-02-23T22:21:10.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17921</th>\n",
              "      <td>NaN</td>\n",
              "      <td>bacc65cf28b9f95b129e9adede43f684f2c5ced3</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1388053126</td>\n",
              "      <td>0</td>\n",
              "      <td>2ba370be80abcb7e462e3c890b7db348b23696fd</td>\n",
              "      <td>Removed slack time in file stream and added be...</td>\n",
              "      <td>SPARK-3401</td>\n",
              "      <td>12739030.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-06T23:12:44.000+0000</td>\n",
              "      <td>2014-09-04T06:48:02.000+0000</td>\n",
              "      <td>2014-09-06T23:12:43.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17922</th>\n",
              "      <td>NaN</td>\n",
              "      <td>d4dfab503a9222b5acf5c4bf69b91c16f298e4aa</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1387922473</td>\n",
              "      <td>-480</td>\n",
              "      <td>b04fff3dd233e23122ac8f1a0072be8bea0961b9</td>\n",
              "      <td>Fixed Python API for sc.setCheckpointDir. Also...</td>\n",
              "      <td>SPARK-3400</td>\n",
              "      <td>12739029.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>ankurd</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>ankurd</td>\n",
              "      <td>Bug</td>\n",
              "      <td>ankurd</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-22T01:28:42.000+0000</td>\n",
              "      <td>2014-09-04T06:43:43.000+0000</td>\n",
              "      <td>2014-09-04T06:50:40.000+0000</td>\n",
              "      <td>GraphX</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17923</th>\n",
              "      <td>NaN</td>\n",
              "      <td>9f79fd89dc84cda7ebeb98a0b43c8e982fefa787</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1387913897</td>\n",
              "      <td>-480</td>\n",
              "      <td>a83e0237d6f3a8745924b05332e25d5772aa7d60</td>\n",
              "      <td>Merge branch 'apache-master' into filestream-f...</td>\n",
              "      <td>SPARK-3399</td>\n",
              "      <td>12739024.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-05T18:07:10.000+0000</td>\n",
              "      <td>2014-09-04T06:33:15.000+0000</td>\n",
              "      <td>2014-09-05T18:07:10.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17927</th>\n",
              "      <td>NaN</td>\n",
              "      <td>19d1d58b67a767b227e009ab8723efaa7087dd07</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1387842523</td>\n",
              "      <td>0</td>\n",
              "      <td>b075af9a176d58a49f925902d6e599429a6c81d4</td>\n",
              "      <td>Fixed bug in file stream that prevented some f...</td>\n",
              "      <td>SPARK-3395</td>\n",
              "      <td>12739000.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-22T07:06:13.000+0000</td>\n",
              "      <td>2014-09-04T02:03:20.000+0000</td>\n",
              "      <td>2014-09-10T06:49:46.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17928</th>\n",
              "      <td>NaN</td>\n",
              "      <td>e7b62cbfbfdb8fda880548bce4249672c6a0a851</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1387766976</td>\n",
              "      <td>-480</td>\n",
              "      <td>2ec7da067fbcb5dcae1a3be696a6023a47f2f19e</td>\n",
              "      <td>Updated CheckpointWriter and FileInputDStream ...</td>\n",
              "      <td>SPARK-3394</td>\n",
              "      <td>12738999.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Closed</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-09T02:30:47.000+0000</td>\n",
              "      <td>2014-09-04T01:54:41.000+0000</td>\n",
              "      <td>2014-09-08T00:58:44.000+0000</td>\n",
              "      <td>Spark Core,SQL</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17930</th>\n",
              "      <td>NaN</td>\n",
              "      <td>984c5824876e0daceb8a74af57593926faf727ce</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1387480848</td>\n",
              "      <td>-480</td>\n",
              "      <td>48be0648167683de8dd064b17513c81956bbffb5</td>\n",
              "      <td>Merge branch 'scheduler-update' into filestrea...</td>\n",
              "      <td>SPARK-3392</td>\n",
              "      <td>12738993.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Trivial</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>Bug</td>\n",
              "      <td>chenghao</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-05T02:17:38.000+0000</td>\n",
              "      <td>2014-09-04T01:41:03.000+0000</td>\n",
              "      <td>2014-09-05T02:17:38.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17932</th>\n",
              "      <td>NaN</td>\n",
              "      <td>50e3b8ec4c8150f1cfc6b92f8871f520adf2cfda</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>1388418266</td>\n",
              "      <td>-480</td>\n",
              "      <td>ede8c631c6d7a940c1ab1629574ec1003eb0861e</td>\n",
              "      <td>Merge pull request #308 from kayousterhout/sta...</td>\n",
              "      <td>SPARK-3390</td>\n",
              "      <td>12738986.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>vidaha</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>Bug</td>\n",
              "      <td>vidaha</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-11T22:25:08.000+0000</td>\n",
              "      <td>2014-09-04T00:48:10.000+0000</td>\n",
              "      <td>2014-09-11T22:25:08.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17935</th>\n",
              "      <td>NaN</td>\n",
              "      <td>19672dca32e41b63dce88b682690ac256b536c8f</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>1388180230</td>\n",
              "      <td>-480</td>\n",
              "      <td>dd6aecb255473e9a3dae9d70f75e780263ef5225</td>\n",
              "      <td>Merge pull request #305 from kayousterhout/lin...</td>\n",
              "      <td>SPARK-3387</td>\n",
              "      <td>12738913.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>christianchua</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>christianchua</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-28T19:09:29.000+0000</td>\n",
              "      <td>2014-09-03T20:48:07.000+0000</td>\n",
              "      <td>2015-02-28T19:09:29.000+0000</td>\n",
              "      <td>Web UI</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17938</th>\n",
              "      <td>NaN</td>\n",
              "      <td>7be1e57786005e52b71238425d4557bb6d1311cc</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1388137300</td>\n",
              "      <td>-600</td>\n",
              "      <td>3c3f3557e9b7e11e712fc0a571fe2a44ae7b3cc0</td>\n",
              "      <td>Merge pull request #298 from aarondav/minor\\n\\...</td>\n",
              "      <td>SPARK-3384</td>\n",
              "      <td>12738879.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rnowling</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>rnowling</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-01-23T18:54:48.000+0000</td>\n",
              "      <td>2014-09-03T19:05:19.000+0000</td>\n",
              "      <td>2015-01-23T18:54:48.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17944</th>\n",
              "      <td>NaN</td>\n",
              "      <td>56094bcd8d3ba3442b88af01393d06fd7cd79bde</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@databricks.com</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@databricks.com</td>\n",
              "      <td>1387995273</td>\n",
              "      <td>-300</td>\n",
              "      <td>e140b00329def4847e43d39cdef089fc7b8f6e74</td>\n",
              "      <td>Merge pull request #290 from ash211/patch-3\\n\\...</td>\n",
              "      <td>SPARK-3378</td>\n",
              "      <td>12738841.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Trivial</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-04T22:07:24.000+0000</td>\n",
              "      <td>2014-09-03T16:50:50.000+0000</td>\n",
              "      <td>2014-09-04T22:07:24.000+0000</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17945</th>\n",
              "      <td>NaN</td>\n",
              "      <td>3665c722b5b540ec96463031b9980cdc43829deb</td>\n",
              "      <td>Andrew Ash</td>\n",
              "      <td>andrew@andrewash.com</td>\n",
              "      <td>Andrew Ash</td>\n",
              "      <td>andrew@andrewash.com</td>\n",
              "      <td>1387934704</td>\n",
              "      <td>-480</td>\n",
              "      <td>d8b09796c5be8740ff0f0f761c58a5d38ee10bfa</td>\n",
              "      <td>Typo: avaiable -&gt; available</td>\n",
              "      <td>SPARK-3377</td>\n",
              "      <td>12738825.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Critical</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-11T21:06:23.000+0000</td>\n",
              "      <td>2014-09-03T15:30:15.000+0000</td>\n",
              "      <td>2014-11-10T21:16:48.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17947</th>\n",
              "      <td>NaN</td>\n",
              "      <td>66b7bea7f82efa4f52186d15824d31035be253de</td>\n",
              "      <td>azuryyu</td>\n",
              "      <td>azuryyyu@gmail.com</td>\n",
              "      <td>azuryyu</td>\n",
              "      <td>azuryyyu@gmail.com</td>\n",
              "      <td>1387880209</td>\n",
              "      <td>480</td>\n",
              "      <td>a783c1238cc60a5bb0605d9c9e3d1bca6da1439d</td>\n",
              "      <td>Make App report interval configurable during '...</td>\n",
              "      <td>SPARK-3375</td>\n",
              "      <td>12738819.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>tgraves</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>tgraves</td>\n",
              "      <td>Bug</td>\n",
              "      <td>tgraves</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-05T14:56:41.000+0000</td>\n",
              "      <td>2014-09-03T15:13:02.000+0000</td>\n",
              "      <td>2014-09-05T14:56:41.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17950</th>\n",
              "      <td>NaN</td>\n",
              "      <td>9cbcf81453a9afca58645969c1bc3ff366392734</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387998762</td>\n",
              "      <td>-300</td>\n",
              "      <td>1c6f6db11ce27761fd575e1e216cddc29c64f877</td>\n",
              "      <td>Remove commented code in __init__.py.\\n</td>\n",
              "      <td>SPARK-3372</td>\n",
              "      <td>12738767.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-04T03:48:12.000+0000</td>\n",
              "      <td>2014-09-03T09:45:57.000+0000</td>\n",
              "      <td>2014-09-04T03:47:34.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17951</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5e71354cb7ff758d9a70494ca1788aebea1bbb08</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387998655</td>\n",
              "      <td>-300</td>\n",
              "      <td>e3ab865be640fffad65300e055edbf3cde38f615</td>\n",
              "      <td>Fix copypasta in __init__.py.  Don't import an...</td>\n",
              "      <td>SPARK-3371</td>\n",
              "      <td>12738760.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>pllee</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>ravi.pesala</td>\n",
              "      <td>Bug</td>\n",
              "      <td>pllee</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-10-13T19:11:15.000+0000</td>\n",
              "      <td>2014-09-03T09:24:53.000+0000</td>\n",
              "      <td>2014-10-02T06:54:23.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17954</th>\n",
              "      <td>NaN</td>\n",
              "      <td>05163057a1810f0a32b722e8c93e5435240636d9</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387948085</td>\n",
              "      <td>-300</td>\n",
              "      <td>861c55ac76964502cba2e7e330e3630a4d279e4b</td>\n",
              "      <td>Split the mllib bindings into a whole bunch of...</td>\n",
              "      <td>SPARK-3368</td>\n",
              "      <td>12738735.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>grahamdennis</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>grahamdennis</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-01-09T12:56:59.000+0000</td>\n",
              "      <td>2014-09-03T07:54:11.000+0000</td>\n",
              "      <td>2016-01-09T12:56:59.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17957</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58e2a7d6d4f036b20896674b1cac076d8daa55e8</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387921720</td>\n",
              "      <td>-300</td>\n",
              "      <td>909280c159f5a9348b4d20ace000594ae853f2a9</td>\n",
              "      <td>Move PythonMLLibAPI into its own package.\\n</td>\n",
              "      <td>SPARK-3365</td>\n",
              "      <td>12738722.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Bug</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-02-13T06:19:33.000+0000</td>\n",
              "      <td>2014-09-03T06:12:00.000+0000</td>\n",
              "      <td>2015-02-13T06:19:33.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17958</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2402180b32d530319d0526490afa3cfafc5c36b8</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387919913</td>\n",
              "      <td>-300</td>\n",
              "      <td>ead89c4735bc20ca7dc89c0814a4fd11c380dfd7</td>\n",
              "      <td>Fix error message ugliness.\\n</td>\n",
              "      <td>SPARK-3364</td>\n",
              "      <td>12738709.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>kallsu</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>kallsu</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-10T07:12:31.000+0000</td>\n",
              "      <td>2014-09-03T05:01:58.000+0000</td>\n",
              "      <td>2014-09-10T07:12:31.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17959</th>\n",
              "      <td>NaN</td>\n",
              "      <td>cbb28111896844a0fd94346cd9c6f9926c706555</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387742638</td>\n",
              "      <td>-300</td>\n",
              "      <td>4480c25e10c121bbda96fea44816cafe4925bf28</td>\n",
              "      <td>Release JVM reference to the ALSModel when don...</td>\n",
              "      <td>SPARK-3363</td>\n",
              "      <td>12738697.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-10T17:49:22.000+0000</td>\n",
              "      <td>2014-09-03T04:26:18.000+0000</td>\n",
              "      <td>2014-09-10T17:49:22.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17960</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20f85eca3d924aecd0fcf61cd516d9ac8e369dc1</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387655653</td>\n",
              "      <td>-300</td>\n",
              "      <td>2122eeebff4fb21339ff289547734f2151faf999</td>\n",
              "      <td>Java stubs for ALSModel.\\n</td>\n",
              "      <td>SPARK-3362</td>\n",
              "      <td>12738688.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>adrian-wang</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-09-10T17:47:14.000+0000</td>\n",
              "      <td>2014-09-03T03:15:00.000+0000</td>\n",
              "      <td>2014-09-10T17:47:14.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17963</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0b494c21675b6cc3b5d669dbd9b9a8f277216613</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387523155</td>\n",
              "      <td>-300</td>\n",
              "      <td>9c33325ca1cc3430a4805afb5a1ccb1f25134f13</td>\n",
              "      <td>Un-semicolon mllib.py.\\n</td>\n",
              "      <td>SPARK-3359</td>\n",
              "      <td>12738665.0</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>mengxr</td>\n",
              "      <td>Minor</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>mengxr</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-15T11:25:10.000+0000</td>\n",
              "      <td>2014-09-03T01:04:40.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17964</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0a5cacb9615d960c93bca8cc3f4ad2a599f94ec0</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>Tor Myklebust</td>\n",
              "      <td>tmyklebu@gmail.com</td>\n",
              "      <td>1387523115</td>\n",
              "      <td>-300</td>\n",
              "      <td>a9097e1d799b81a66cdc67a84322da5e0ba2fb9d</td>\n",
              "      <td>Change some docstrings and add some others.\\n</td>\n",
              "      <td>SPARK-3358</td>\n",
              "      <td>12738660.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2014-12-13T07:33:17.000+0000</td>\n",
              "      <td>2014-09-03T00:37:27.000+0000</td>\n",
              "      <td>2014-12-13T07:33:17.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7902 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               key  ... severity\n",
              "1      SPARK-18296  ...      1.0\n",
              "2      SPARK-18167  ...     10.0\n",
              "4      SPARK-18269  ...      3.0\n",
              "5      SPARK-18210  ...      3.0\n",
              "6      SPARK-17854  ...      3.0\n",
              "8              NaN  ...      1.0\n",
              "13     SPARK-18101  ...      3.0\n",
              "14     SPARK-17183  ...      3.0\n",
              "16     SPARK-18260  ...      3.0\n",
              "18     SPARK-18189  ...     10.0\n",
              "22     SPARK-18197  ...      3.0\n",
              "24     SPARK-18200  ...      3.0\n",
              "28     SPARK-18257  ...      3.0\n",
              "29             NaN  ...      3.0\n",
              "30     SPARK-18212  ...      3.0\n",
              "34     SPARK-18244  ...      5.0\n",
              "35     SPARK-17949  ...      5.0\n",
              "36     SPARK-17957  ...      3.0\n",
              "43     SPARK-18175  ...      1.0\n",
              "44     SPARK-17963  ...      3.0\n",
              "47     SPARK-18214  ...      1.0\n",
              "48     SPARK-17058  ...      3.0\n",
              "49     SPARK-18160  ...     10.0\n",
              "50     SPARK-14393  ...      3.0\n",
              "51     SPARK-17895  ...      3.0\n",
              "52     SPARK-17683  ...      5.0\n",
              "54     SPARK-18076  ...      5.0\n",
              "56     SPARK-18198  ...      1.0\n",
              "57             NaN  ...      1.0\n",
              "65     SPARK-17838  ...      3.0\n",
              "...            ...  ...      ...\n",
              "17905          NaN  ...      1.0\n",
              "17907          NaN  ...      3.0\n",
              "17908          NaN  ...      5.0\n",
              "17909          NaN  ...      3.0\n",
              "17910          NaN  ...      1.0\n",
              "17914          NaN  ...      3.0\n",
              "17916          NaN  ...      1.0\n",
              "17918          NaN  ...      5.0\n",
              "17919          NaN  ...      3.0\n",
              "17921          NaN  ...      3.0\n",
              "17922          NaN  ...     10.0\n",
              "17923          NaN  ...      3.0\n",
              "17927          NaN  ...      1.0\n",
              "17928          NaN  ...      1.0\n",
              "17930          NaN  ...      0.0\n",
              "17932          NaN  ...      5.0\n",
              "17935          NaN  ...      3.0\n",
              "17938          NaN  ...      3.0\n",
              "17944          NaN  ...      0.0\n",
              "17945          NaN  ...      5.0\n",
              "17947          NaN  ...     10.0\n",
              "17950          NaN  ...     10.0\n",
              "17951          NaN  ...      3.0\n",
              "17954          NaN  ...      3.0\n",
              "17957          NaN  ...     10.0\n",
              "17958          NaN  ...      3.0\n",
              "17959          NaN  ...      3.0\n",
              "17960          NaN  ...      3.0\n",
              "17963          NaN  ...      1.0\n",
              "17964          NaN  ...      3.0\n",
              "\n",
              "[7902 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EVqich7hxfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['created'] = pd.to_datetime(df['created'], errors='coerce', utc='Europe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mQUiYNdh1F0",
        "colab_type": "code",
        "outputId": "fac2252f-d79b-4303-8e81-2556feaa90f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "april = pd.to_datetime('01 April 2016', utc='Europe')\n",
        "october = pd.to_datetime('1 October 2016', utc='Europe') \n",
        "df[(df['created'] > april) & (df['created'] < october) & (df['severity'] == 10)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>cid</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>time</th>\n",
              "      <th>time_offset</th>\n",
              "      <th>tree_id</th>\n",
              "      <th>message_encoding</th>\n",
              "      <th>key</th>\n",
              "      <th>jira_id</th>\n",
              "      <th>status_category</th>\n",
              "      <th>creator</th>\n",
              "      <th>priority</th>\n",
              "      <th>status</th>\n",
              "      <th>assignee</th>\n",
              "      <th>issuetype</th>\n",
              "      <th>reporter</th>\n",
              "      <th>resolution</th>\n",
              "      <th>project</th>\n",
              "      <th>updated</th>\n",
              "      <th>created</th>\n",
              "      <th>resolved</th>\n",
              "      <th>components</th>\n",
              "      <th>severity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>SPARK-14525</td>\n",
              "      <td>aa3a6841ebaf45efb5d3930a93869948bdd0d2b6</td>\n",
              "      <td>hyukjinkwon</td>\n",
              "      <td>gurwls223@gmail.com</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>1475862752</td>\n",
              "      <td>-420</td>\n",
              "      <td>accbb233c9e1b621d9f2958cb51f6d7b1e3b52cc</td>\n",
              "      <td>[SPARK-14525][SQL][FOLLOWUP] Clean up JdbcRela...</td>\n",
              "      <td>SPARK-14462</td>\n",
              "      <td>12956988.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-04-11T16:35:57.000+0000</td>\n",
              "      <td>2016-04-07 19:57:57+00:00</td>\n",
              "      <td>2016-04-11T16:35:57.000+0000</td>\n",
              "      <td>ML,MLlib</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>NaN</td>\n",
              "      <td>958200497affb40f05e321c2b0e252d365ae02f4</td>\n",
              "      <td>José Hiram Soltren</td>\n",
              "      <td>jose@cloudera.com</td>\n",
              "      <td>Marcelo Vanzin</td>\n",
              "      <td>vanzin@cloudera.com</td>\n",
              "      <td>1475169536</td>\n",
              "      <td>-420</td>\n",
              "      <td>2c60f4befbabc179feebb2e286b5c868096689e3</td>\n",
              "      <td>[DOCS] Reorganize explanation of Accumulators ...</td>\n",
              "      <td>SPARK-14393</td>\n",
              "      <td>12956001.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>jpiper</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>mengxr</td>\n",
              "      <td>Bug</td>\n",
              "      <td>jpiper</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-11-03T05:26:03.000+0000</td>\n",
              "      <td>2016-04-05 01:04:15+00:00</td>\n",
              "      <td>2016-11-02T18:41:43.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>SPARK-17356</td>\n",
              "      <td>6f13aa7dfee12b1b301bd10a1050549008ecc67e</td>\n",
              "      <td>Sean Zhong</td>\n",
              "      <td>seanzhong@databricks.com</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>1473149150</td>\n",
              "      <td>480</td>\n",
              "      <td>67f7324e327eabf40d8a0970cd0baaea7994d666</td>\n",
              "      <td>[SPARK-17356][SQL] Fix out of memory issue whe...</td>\n",
              "      <td>SPARK-16379</td>\n",
              "      <td>12986787.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>skonto</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>skonto</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-06T21:21:41.000+0000</td>\n",
              "      <td>2016-07-05 11:24:05+00:00</td>\n",
              "      <td>2016-07-06T20:36:02.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1b001b5203444cc8d5c4887a30e03e8fb298d17d</td>\n",
              "      <td>Yanbo Liang</td>\n",
              "      <td>ybliang8@gmail.com</td>\n",
              "      <td>Yanbo Liang</td>\n",
              "      <td>ybliang8@gmail.com</td>\n",
              "      <td>1472992727</td>\n",
              "      <td>-420</td>\n",
              "      <td>c46d3e52bbba448c2fb84f47e866c56041c7cb14</td>\n",
              "      <td>[MINOR][ML][MLLIB] Remove work around for bree...</td>\n",
              "      <td>SPARK-16371</td>\n",
              "      <td>12986683.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>maver1ck</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>hyukjin.kwon</td>\n",
              "      <td>Bug</td>\n",
              "      <td>maver1ck</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-06T19:46:06.000+0000</td>\n",
              "      <td>2016-07-04 22:50:22+00:00</td>\n",
              "      <td>2016-07-06T19:42:09.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706</th>\n",
              "      <td>SPARK-17301</td>\n",
              "      <td>48b459ddd58affd5519856cb6e204398b7739a2a</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1472522280</td>\n",
              "      <td>480</td>\n",
              "      <td>7533ffd9546261dff30edb0a193a7a5f32b3ce95</td>\n",
              "      <td>[SPARK-17301][SQL] Remove unused classTag fiel...</td>\n",
              "      <td>SPARK-15386</td>\n",
              "      <td>12970835.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>vanzin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>vanzin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-18T21:22:49.000+0000</td>\n",
              "      <td>2016-05-18 17:07:08+00:00</td>\n",
              "      <td>2016-05-18T21:22:49.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>SPARK-12978</td>\n",
              "      <td>2b0cc4e0dfa4ffb9f21ff4a303015bc9c962d42b</td>\n",
              "      <td>Takeshi YAMAMURO</td>\n",
              "      <td>linguin.m.s@gmail.com</td>\n",
              "      <td>Herman van Hovell</td>\n",
              "      <td>hvanhovell@databricks.com</td>\n",
              "      <td>1472121598</td>\n",
              "      <td>120</td>\n",
              "      <td>fcf6b3fea6c9604bee57830c007f9d82ea6937c3</td>\n",
              "      <td>[SPARK-12978][SQL] Skip unnecessary final grou...</td>\n",
              "      <td>SPARK-15345</td>\n",
              "      <td>12969989.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-08-10T01:05:27.000+0000</td>\n",
              "      <td>2016-05-16 12:39:24+00:00</td>\n",
              "      <td>2016-06-23T03:25:35.000+0000</td>\n",
              "      <td>PySpark,SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>SPARK-17115</td>\n",
              "      <td>8d35a6f68d6d733212674491cbf31bed73fada0f</td>\n",
              "      <td>Davies Liu</td>\n",
              "      <td>davies@databricks.com</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>1471853763</td>\n",
              "      <td>480</td>\n",
              "      <td>4574a1aeef90dab75b1300abb599632cde3973e7</td>\n",
              "      <td>[SPARK-17115][SQL] decrease the threshold when...</td>\n",
              "      <td>SPARK-15289</td>\n",
              "      <td>12968225.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Bug</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-16T08:17:34.000+0000</td>\n",
              "      <td>2016-05-12 07:53:55+00:00</td>\n",
              "      <td>2016-05-12T16:21:55.000+0000</td>\n",
              "      <td>Build,SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>SPARK-17136</td>\n",
              "      <td>287bea13050b8eedc3b8b6b3491f1b5e5bc24d7a</td>\n",
              "      <td>sethah</td>\n",
              "      <td>seth.hendrickson16@gmail.com</td>\n",
              "      <td>DB Tsai</td>\n",
              "      <td>dbt@netflix.com</td>\n",
              "      <td>1471583808</td>\n",
              "      <td>-420</td>\n",
              "      <td>06f68fcbd5774ede799a8f3633377f1298c5303e</td>\n",
              "      <td>[SPARK-7159][ML] Add multiclass logistic regre...</td>\n",
              "      <td>SPARK-14762</td>\n",
              "      <td>12960417.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>davies</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>hvanhovell</td>\n",
              "      <td>Bug</td>\n",
              "      <td>davies</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-11-03T01:19:58.000+0000</td>\n",
              "      <td>2016-04-20 17:40:36+00:00</td>\n",
              "      <td>2016-04-22T18:38:54.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>SPARK-16941</td>\n",
              "      <td>a45fefd17ec4a499b988a2f9931ce397918d3bef</td>\n",
              "      <td>huangzhaowei</td>\n",
              "      <td>carlmartinmax@gmail.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1470911308</td>\n",
              "      <td>60</td>\n",
              "      <td>58b6ee26985c565327d56a8614d4574583fee866</td>\n",
              "      <td>[SPARK-16941] Use concurrentHashMap instead of...</td>\n",
              "      <td>SPARK-14689</td>\n",
              "      <td>12959476.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>kiszk</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>kiszk</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-04-20T20:44:13.000+0000</td>\n",
              "      <td>2016-04-17 18:14:20+00:00</td>\n",
              "      <td>2016-04-20T20:44:12.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>SPARK-16053</td>\n",
              "      <td>b0f2fb5b9729b38744bf784f2072f5ee52314f87</td>\n",
              "      <td>Dongjoon Hyun</td>\n",
              "      <td>dongjoon@apache.org</td>\n",
              "      <td>Shivaram Venkataraman</td>\n",
              "      <td>shivaram@cs.berkeley.edu</td>\n",
              "      <td>1466455263</td>\n",
              "      <td>-420</td>\n",
              "      <td>bbe026f28c48cdd9741016d0ac19d6abb1639df8</td>\n",
              "      <td>[SPARK-16053][R] Add `spark_partition_id` in S...</td>\n",
              "      <td>SPARK-15581</td>\n",
              "      <td>12973191.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>Umbrella</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-11-05T01:22:21.000+0000</td>\n",
              "      <td>2016-05-26 20:36:52+00:00</td>\n",
              "      <td>2016-11-02T00:24:00.000+0000</td>\n",
              "      <td>ML,MLlib</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>SPARK-15782</td>\n",
              "      <td>4df8df5c2e68f5a5d231c401b04d762d7a648159</td>\n",
              "      <td>Nezih Yigitbasi</td>\n",
              "      <td>nyigitbasi@netflix.com</td>\n",
              "      <td>Marcelo Vanzin</td>\n",
              "      <td>vanzin@cloudera.com</td>\n",
              "      <td>1466024856</td>\n",
              "      <td>-420</td>\n",
              "      <td>3d5ff81f6e22fa75167e92ede32a53bbf3887545</td>\n",
              "      <td>[SPARK-15782][YARN] Set spark.jars system prop...</td>\n",
              "      <td>SPARK-15501</td>\n",
              "      <td>12972323.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>mlnick</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>mlnick</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>mlnick</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-17T18:10:13.000+0000</td>\n",
              "      <td>2016-05-24 09:00:17+00:00</td>\n",
              "      <td>2016-06-17T12:36:53.000+0000</td>\n",
              "      <td>Documentation,ML</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>SPARK-15092</td>\n",
              "      <td>72353311d3a37cb523c5bdd8072ffdff99af9749</td>\n",
              "      <td>Holden Karau</td>\n",
              "      <td>holden@us.ibm.com</td>\n",
              "      <td>Nick Pentreath</td>\n",
              "      <td>nickp@za.ibm.com</td>\n",
              "      <td>1464908114</td>\n",
              "      <td>-420</td>\n",
              "      <td>45a2def7a6f97388e468203cd87c846ae28ce9b6</td>\n",
              "      <td>[SPARK-15092][SPARK-15139][PYSPARK][ML] Pyspar...</td>\n",
              "      <td>SPARK-16224</td>\n",
              "      <td>12983400.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>Bug</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-30T11:32:54.000+0000</td>\n",
              "      <td>2016-06-27 07:32:46+00:00</td>\n",
              "      <td>2016-06-28T14:55:18.000+0000</td>\n",
              "      <td>PySpark</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1780</th>\n",
              "      <td>SPARK-15076</td>\n",
              "      <td>63b7f127caf2fdf96eeb8457afd6c96bc8309a58</td>\n",
              "      <td>Dongjoon Hyun</td>\n",
              "      <td>dongjoon@apache.org</td>\n",
              "      <td>Wenchen Fan</td>\n",
              "      <td>wenchen@databricks.com</td>\n",
              "      <td>1464886138</td>\n",
              "      <td>-420</td>\n",
              "      <td>117bc8d83080df4dec464edbfdbc9664ef158e25</td>\n",
              "      <td>[SPARK-15076][SQL] Add ReorderAssociativeOpera...</td>\n",
              "      <td>SPARK-16216</td>\n",
              "      <td>12983231.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>hyukjin.kwon</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>hyukjin.kwon</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>hyukjin.kwon</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-21T21:24:49.000+0000</td>\n",
              "      <td>2016-06-26 05:54:08+00:00</td>\n",
              "      <td>2016-08-25T04:19:34.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2808</th>\n",
              "      <td>SPARK-14470</td>\n",
              "      <td>583b5e05309adb73cdffd974a810d6bfb5f2ff95</td>\n",
              "      <td>Aaron Tokhy</td>\n",
              "      <td>tokaaron@amazon.com</td>\n",
              "      <td>Sean Owen</td>\n",
              "      <td>sowen@cloudera.com</td>\n",
              "      <td>1460116345</td>\n",
              "      <td>60</td>\n",
              "      <td>cd7826f1f3d51f1ce7bd2e0977b970730b2dbf36</td>\n",
              "      <td>[SPARK-14470] Allow for overriding both httpcl...</td>\n",
              "      <td>SPARK-17289</td>\n",
              "      <td>13000919.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>clockfly</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>maropu</td>\n",
              "      <td>Bug</td>\n",
              "      <td>clockfly</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-01T05:23:50.000+0000</td>\n",
              "      <td>2016-08-29 10:25:15+00:00</td>\n",
              "      <td>2016-09-01T05:23:48.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2830</th>\n",
              "      <td>SPARK-12610</td>\n",
              "      <td>d76592276f9f66fed8012d876595de8717f516a9</td>\n",
              "      <td>Herman van Hovell</td>\n",
              "      <td>hvanhovell@questtec.nl</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1459995910</td>\n",
              "      <td>-420</td>\n",
              "      <td>bb3570eac8b6885efe77677d18cda30df7cb0a69</td>\n",
              "      <td>[SPARK-12610][SQL] Left Anti Join\\n\\n### What ...</td>\n",
              "      <td>SPARK-17267</td>\n",
              "      <td>13000536.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>petermaxlee</td>\n",
              "      <td>Bug</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-11-02T06:00:11.000+0000</td>\n",
              "      <td>2016-08-26 23:05:21+00:00</td>\n",
              "      <td>2016-11-02T05:59:48.000+0000</td>\n",
              "      <td>Structured Streaming</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3273</th>\n",
              "      <td>SPARK-13658</td>\n",
              "      <td>6a4bfcd62b7effcfbb865bdd301d41a0ba6e5c94</td>\n",
              "      <td>Liang-Chi Hsieh</td>\n",
              "      <td>simonh@tw.ibm.com</td>\n",
              "      <td>Michael Armbrust</td>\n",
              "      <td>michael@databricks.com</td>\n",
              "      <td>1457979809</td>\n",
              "      <td>-420</td>\n",
              "      <td>c37b50e5cc6c436302ea84c92aa4ecef63c29a97</td>\n",
              "      <td>[SPARK-13658][SQL] BooleanSimplification rule ...</td>\n",
              "      <td>SPARK-16121</td>\n",
              "      <td>12981534.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>Bug</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-19T12:33:02.000+0000</td>\n",
              "      <td>2016-06-21 22:16:18+00:00</td>\n",
              "      <td>2016-06-22T10:09:57.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3523</th>\n",
              "      <td>SPARK-13292</td>\n",
              "      <td>35316cb0b744bef9bcb390411ddc321167f953be</td>\n",
              "      <td>Yu ISHIKAWA</td>\n",
              "      <td>yuu.ishikawa@gmail.com</td>\n",
              "      <td>Xiangrui Meng</td>\n",
              "      <td>meng@databricks.com</td>\n",
              "      <td>1456435750</td>\n",
              "      <td>-480</td>\n",
              "      <td>3ed75c8c6e20cb0a5468925416f70cd86a2933c2</td>\n",
              "      <td>[SPARK-13292] [ML] [PYTHON] QuantileDiscretize...</td>\n",
              "      <td>SPARK-15062</td>\n",
              "      <td>12964305.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>koert</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>mengbo</td>\n",
              "      <td>Bug</td>\n",
              "      <td>koert</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-06T19:01:06.000+0000</td>\n",
              "      <td>2016-05-02 14:14:36+00:00</td>\n",
              "      <td>2016-05-03T01:20:53.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3539</th>\n",
              "      <td>SPARK-13482</td>\n",
              "      <td>264533b553be806b6c45457201952e83c028ec78</td>\n",
              "      <td>huangzhaowei</td>\n",
              "      <td>carlmartinmax@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1456386737</td>\n",
              "      <td>-480</td>\n",
              "      <td>6d4c4bb595de5d698f08bfa1e4ce734e96eaf7a6</td>\n",
              "      <td>[SPARK-13482][MINOR][CONFIGURATION] Make consi...</td>\n",
              "      <td>SPARK-15046</td>\n",
              "      <td>12964163.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>tleftwich</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>vanzin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>tleftwich</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-15T14:10:21.000+0000</td>\n",
              "      <td>2016-05-01 19:14:06+00:00</td>\n",
              "      <td>2016-06-15T14:10:21.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3575</th>\n",
              "      <td>SPARK-13236</td>\n",
              "      <td>01e10c9fef51c69ecf3060929c62d113cfc672b9</td>\n",
              "      <td>gatorsmile</td>\n",
              "      <td>gatorsmile@gmail.com</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>1456211819</td>\n",
              "      <td>480</td>\n",
              "      <td>303af42f70b2119909466d3f17f34187640ac240</td>\n",
              "      <td>[SPARK-13236] SQL Generation for Set Operation...</td>\n",
              "      <td>SPARK-15010</td>\n",
              "      <td>12963939.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>mengxr</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>cloud_fan</td>\n",
              "      <td>Bug</td>\n",
              "      <td>mengxr</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-04-30T02:01:21.000+0000</td>\n",
              "      <td>2016-04-29 19:04:05+00:00</td>\n",
              "      <td>2016-04-30T02:01:21.000+0000</td>\n",
              "      <td>Spark Core,Spark Shell</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>SPARK-11469</td>\n",
              "      <td>6091e91fca58078a0f1d9c35d68c0ae7205a534c</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1446772235</td>\n",
              "      <td>-480</td>\n",
              "      <td>eb2f68895f8dc7d8b52572083ca84580c5f37059</td>\n",
              "      <td>Revert \"[SPARK-11469][SQL] Allow users to defi...</td>\n",
              "      <td>SPARK-14959</td>\n",
              "      <td>12963109.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>syepes</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>xwu0226</td>\n",
              "      <td>Bug</td>\n",
              "      <td>syepes</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-21T23:54:25.000+0000</td>\n",
              "      <td>2016-04-27 15:09:23+00:00</td>\n",
              "      <td>2016-06-03T05:49:48.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6075</th>\n",
              "      <td>SPARK-9903</td>\n",
              "      <td>d7053bea985679c514b3add029631ea23e1730ce</td>\n",
              "      <td>Xiangrui Meng</td>\n",
              "      <td>meng@databricks.com</td>\n",
              "      <td>Xiangrui Meng</td>\n",
              "      <td>meng@databricks.com</td>\n",
              "      <td>1439437480</td>\n",
              "      <td>-420</td>\n",
              "      <td>287efbc8452d90c06a8f86720636ad42b21c3fc3</td>\n",
              "      <td>[SPARK-9903] [MLLIB] skip local processing in ...</td>\n",
              "      <td>SPARK-16018</td>\n",
              "      <td>12980218.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>Dhruve Ashar</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Dhruve Ashar</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Dhruve Ashar</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-17T20:45:39.000+0000</td>\n",
              "      <td>2016-06-17 15:32:17+00:00</td>\n",
              "      <td>2016-06-17T20:45:38.000+0000</td>\n",
              "      <td>Shuffle</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6110</th>\n",
              "      <td>SPARK-9806</td>\n",
              "      <td>a807fcbe50b2ce18751d80d39e9d21842f7da32a</td>\n",
              "      <td>Rohit Agarwal</td>\n",
              "      <td>rohita@qubole.com</td>\n",
              "      <td>Andrew Or</td>\n",
              "      <td>andrew@databricks.com</td>\n",
              "      <td>1439360439</td>\n",
              "      <td>-420</td>\n",
              "      <td>04367860c6f6163c54e4ed7ad25ad835c3b2028c</td>\n",
              "      <td>[SPARK-9806] [WEB UI] Don't share ReplayListen...</td>\n",
              "      <td>SPARK-15782</td>\n",
              "      <td>12976134.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>nezihyigitbasi</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>nezihyigitbasi</td>\n",
              "      <td>Bug</td>\n",
              "      <td>nezihyigitbasi</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-17T01:21:17.000+0000</td>\n",
              "      <td>2016-06-06 16:53:42+00:00</td>\n",
              "      <td>2016-06-17T01:21:17.000+0000</td>\n",
              "      <td>YARN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6804</th>\n",
              "      <td>SPARK-8221</td>\n",
              "      <td>a9385271a9f6b97ec6aa619cf56ee556ba2fb0de</td>\n",
              "      <td>zhichao.li</td>\n",
              "      <td>zhichao.li@intel.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1436982218</td>\n",
              "      <td>-420</td>\n",
              "      <td>ee3d548fcf187df1445d5b91e68d7511a6d17e89</td>\n",
              "      <td>[SPARK-8221][SQL]Add pmod function\\n\\nhttps://...</td>\n",
              "      <td>SPARK-17093</td>\n",
              "      <td>12997715.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>proflin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-08-31T21:16:07.000+0000</td>\n",
              "      <td>2016-08-16 21:25:46+00:00</td>\n",
              "      <td>2016-08-25T12:20:39.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7006</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2b820f2a4bf9b154762e7516a5b0485322799da9</td>\n",
              "      <td>Liang-Chi Hsieh</td>\n",
              "      <td>viirya@gmail.com</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>1436075570</td>\n",
              "      <td>-420</td>\n",
              "      <td>f2847ca351b7f25d0d6f94194de586572b2a9404</td>\n",
              "      <td>[MINOR] [SQL] Minor fix for CatalystSchemaConv...</td>\n",
              "      <td>SPARK-17391</td>\n",
              "      <td>13002576.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Bug</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-05T03:19:15.000+0000</td>\n",
              "      <td>2016-09-03 17:38:04+00:00</td>\n",
              "      <td>2016-09-05T03:19:02.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7733</th>\n",
              "      <td>SPARK-7788</td>\n",
              "      <td>1c388a9985999e043fa002618a357bc8f0a8b65a</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1432341541</td>\n",
              "      <td>-420</td>\n",
              "      <td>b35ecb5e872c3ec02a714f1ad320fa1e80890d31</td>\n",
              "      <td>[SPARK-7788] Made KinesisReceiver.onStart() no...</td>\n",
              "      <td>SPARK-16664</td>\n",
              "      <td>12991445.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>skolli</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>darkcaller</td>\n",
              "      <td>Bug</td>\n",
              "      <td>skolli</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-08-29T14:59:14.000+0000</td>\n",
              "      <td>2016-07-21 13:22:10+00:00</td>\n",
              "      <td>2016-07-29T11:28:11.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7777</th>\n",
              "      <td>SPARK-7752</td>\n",
              "      <td>13348e21b6b1c0df42c18b82b86c613291228863</td>\n",
              "      <td>Xiangrui Meng</td>\n",
              "      <td>meng@databricks.com</td>\n",
              "      <td>Joseph K. Bradley</td>\n",
              "      <td>joseph@databricks.com</td>\n",
              "      <td>1432229408</td>\n",
              "      <td>-420</td>\n",
              "      <td>83915ccd4d2ff42e0da660f1f9e26c8c893eaff4</td>\n",
              "      <td>[SPARK-7752] [MLLIB] Use lowercase letters for...</td>\n",
              "      <td>SPARK-16620</td>\n",
              "      <td>12990657.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>proflin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>proflin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>proflin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-19T17:25:53.000+0000</td>\n",
              "      <td>2016-07-19 06:06:59+00:00</td>\n",
              "      <td>2016-07-19T17:25:53.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8507</th>\n",
              "      <td>SPARK-6207</td>\n",
              "      <td>77620be76e82b6cdaae406cd752d3272656f5fe0</td>\n",
              "      <td>Doug Balog</td>\n",
              "      <td>doug.balog@target.com</td>\n",
              "      <td>Thomas Graves</td>\n",
              "      <td>tgraves@apache.org</td>\n",
              "      <td>1428936598</td>\n",
              "      <td>-300</td>\n",
              "      <td>c157aa8a6c1eb394fbec3f63d99550426411882f</td>\n",
              "      <td>[SPARK-6207] [YARN] [SQL] Adds delegation toke...</td>\n",
              "      <td>SPARK-17491</td>\n",
              "      <td>13004049.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-17T18:47:03.000+0000</td>\n",
              "      <td>2016-09-10 06:47:43+00:00</td>\n",
              "      <td>2016-09-17T18:47:01.000+0000</td>\n",
              "      <td>Block Manager</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8556</th>\n",
              "      <td>SPARK-3074</td>\n",
              "      <td>b5c51c8df480f1a82a82e4d597d8eea631bffb4e</td>\n",
              "      <td>Davies Liu</td>\n",
              "      <td>davies.liu@gmail.com</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>1428624443</td>\n",
              "      <td>-420</td>\n",
              "      <td>7842078dd2b5d8dab92a129725353647136e8a9f</td>\n",
              "      <td>[SPARK-3074] [PySpark] support groupByKey() wi...</td>\n",
              "      <td>SPARK-17442</td>\n",
              "      <td>13003400.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>falaki</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>felixcheung</td>\n",
              "      <td>Bug</td>\n",
              "      <td>falaki</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-08T15:23:53.000+0000</td>\n",
              "      <td>2016-09-07 23:03:01+00:00</td>\n",
              "      <td>2016-09-08T15:23:20.000+0000</td>\n",
              "      <td>SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8580</th>\n",
              "      <td>SPARK-6705</td>\n",
              "      <td>d138aa8ee23f4450242da3ac70a493229a90c76b</td>\n",
              "      <td>Omede Firouz</td>\n",
              "      <td>ofirouz@palantir.com</td>\n",
              "      <td>Joseph K. Bradley</td>\n",
              "      <td>joseph@databricks.com</td>\n",
              "      <td>1428464191</td>\n",
              "      <td>-240</td>\n",
              "      <td>059bc1504106aba35d1b6cac5e8d428066f022cd</td>\n",
              "      <td>[SPARK-6705][MLLIB] Add fit intercept api to m...</td>\n",
              "      <td>SPARK-17418</td>\n",
              "      <td>13003023.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>luciano resende</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>luciano resende</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-21T18:43:35.000+0000</td>\n",
              "      <td>2016-09-06 19:46:06+00:00</td>\n",
              "      <td>2016-09-21T18:43:35.000+0000</td>\n",
              "      <td>Build,DStreams</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8593</th>\n",
              "      <td>SPARK-4313</td>\n",
              "      <td>a0846c4b635eac8d8637c83d490177f881952d27</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>1428388396</td>\n",
              "      <td>-420</td>\n",
              "      <td>a8290626a78702e93488fd1c94a22989054b18af</td>\n",
              "      <td>[SPARK-6716] Change SparkContext.DRIVER_IDENTI...</td>\n",
              "      <td>SPARK-17405</td>\n",
              "      <td>13002813.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-08T23:48:08.000+0000</td>\n",
              "      <td>2016-09-05 23:42:11+00:00</td>\n",
              "      <td>2016-09-08T23:48:07.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14796</th>\n",
              "      <td>NaN</td>\n",
              "      <td>886b39de557b4d5f54f5ca11559fca9799534280</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>rosenville@gmail.com</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>rosenville@gmail.com</td>\n",
              "      <td>1345354431</td>\n",
              "      <td>-420</td>\n",
              "      <td>ff4504773f3f75b2408f5acbc1a9e0e0b3b3ff64</td>\n",
              "      <td>Add Python API.\\n</td>\n",
              "      <td>SPARK-15796</td>\n",
              "      <td>12976278.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>gfeher</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>gfeher</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-15T02:53:25.000+0000</td>\n",
              "      <td>2016-06-07 03:14:15+00:00</td>\n",
              "      <td>2016-06-16T21:04:32.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15308</th>\n",
              "      <td>NaN</td>\n",
              "      <td>02a6761589c35f15f1a6e3b63a7964ba057d3ba6</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>1359600755</td>\n",
              "      <td>-480</td>\n",
              "      <td>5349b811fb721d440c77265b3c13061bcdca5982</td>\n",
              "      <td>Merge branch 'master' into blockmanager_info\\n...</td>\n",
              "      <td>SPARK-15177</td>\n",
              "      <td>12965551.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>yanboliang</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yanboliang</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>yanboliang</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-21T15:34:55.000+0000</td>\n",
              "      <td>2016-05-06 10:17:19+00:00</td>\n",
              "      <td>2016-06-21T15:33:55.000+0000</td>\n",
              "      <td>ML,SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15320</th>\n",
              "      <td>NaN</td>\n",
              "      <td>85019d76a44df9c4d19592da1484f925ef5b5b56</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>1359860999</td>\n",
              "      <td>-480</td>\n",
              "      <td>75d3ea27bae9d4a72461cdd651c7acb1cfe364c6</td>\n",
              "      <td>Merge pull request #427 from woggling/dag-sche...</td>\n",
              "      <td>SPARK-15165</td>\n",
              "      <td>12965435.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>Bug</td>\n",
              "      <td>sarutak</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-21T06:37:02.000+0000</td>\n",
              "      <td>2016-05-05 22:28:48+00:00</td>\n",
              "      <td>2016-05-17T17:11:35.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15326</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4bf3d7ea1252454ca584a3dabf26bdeab4069409</td>\n",
              "      <td>Charles Reiss</td>\n",
              "      <td>charles@eecs.berkeley.edu</td>\n",
              "      <td>Charles Reiss</td>\n",
              "      <td>charles@eecs.berkeley.edu</td>\n",
              "      <td>1359515158</td>\n",
              "      <td>-480</td>\n",
              "      <td>3cc054a444769e4a1f4cdc184e0b8deb9cde097a</td>\n",
              "      <td>Clear spark.master.port to cleanup for other t...</td>\n",
              "      <td>SPARK-15159</td>\n",
              "      <td>12965274.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>sunrui</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>felixcheung</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>sunrui</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-18T04:48:03.000+0000</td>\n",
              "      <td>2016-05-05 13:18:01+00:00</td>\n",
              "      <td>2016-06-18T04:36:20.000+0000</td>\n",
              "      <td>SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15356</th>\n",
              "      <td>NaN</td>\n",
              "      <td>934a53c8b677df524315a75011b4c9396eb4b54e</td>\n",
              "      <td>Mark Hamstra</td>\n",
              "      <td>markhamstra@gmail.com</td>\n",
              "      <td>Mark Hamstra</td>\n",
              "      <td>markhamstra@gmail.com</td>\n",
              "      <td>1360131598</td>\n",
              "      <td>-480</td>\n",
              "      <td>273cb2e688e393fb6fc52bab8b7d5bd864459c9c</td>\n",
              "      <td>Change docs on 'reduce' since the merging of l...</td>\n",
              "      <td>SPARK-15129</td>\n",
              "      <td>12965054.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>GayathriMurali</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-18T04:10:48.000+0000</td>\n",
              "      <td>2016-05-04 18:02:59+00:00</td>\n",
              "      <td>2016-06-18T04:10:48.000+0000</td>\n",
              "      <td>Documentation,ML,SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15361</th>\n",
              "      <td>NaN</td>\n",
              "      <td>582d31dff99c161a51e15497db983a4b5a6d4cdb</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>1360617894</td>\n",
              "      <td>-480</td>\n",
              "      <td>a2b221c1d406ee7e5b87dec3ae07140dab7cfd01</td>\n",
              "      <td>Formatting fixes\\n</td>\n",
              "      <td>SPARK-15124</td>\n",
              "      <td>12965048.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yanboliang</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-07-17T04:56:15.000+0000</td>\n",
              "      <td>2016-05-04 17:56:33+00:00</td>\n",
              "      <td>2016-07-13T23:53:28.000+0000</td>\n",
              "      <td>Documentation,SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15373</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ae2234687d9040b42619c374eadfd40c896d386d</td>\n",
              "      <td>Stephen Haberman</td>\n",
              "      <td>stephen@exigencecorp.com</td>\n",
              "      <td>Stephen Haberman</td>\n",
              "      <td>stephen@exigencecorp.com</td>\n",
              "      <td>1361041831</td>\n",
              "      <td>-360</td>\n",
              "      <td>13ab3ae363423b1576be4340b73dfbf634b8f039</td>\n",
              "      <td>Make CoGroupedRDDs explicitly have the same ke...</td>\n",
              "      <td>SPARK-15112</td>\n",
              "      <td>12964888.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>lian cheng</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-30T06:19:35.000+0000</td>\n",
              "      <td>2016-05-04 06:28:25+00:00</td>\n",
              "      <td>2016-05-30T06:19:35.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15385</th>\n",
              "      <td>NaN</td>\n",
              "      <td>d36abdb053be0dae2225b09c747830ac7b64387d</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>1362352846</td>\n",
              "      <td>-480</td>\n",
              "      <td>25b88a9446b4f86c740322575e60404f9bb325b5</td>\n",
              "      <td>Merge branch 'master' into stageInfo\\n</td>\n",
              "      <td>SPARK-15100</td>\n",
              "      <td>12964789.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yuhaoyan</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-21T19:22:11.000+0000</td>\n",
              "      <td>2016-05-03 21:04:18+00:00</td>\n",
              "      <td>2016-06-21T19:22:11.000+0000</td>\n",
              "      <td>Documentation,ML</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15386</th>\n",
              "      <td>NaN</td>\n",
              "      <td>8f17387d9723c1359c86ff1773cf6613f02bb9c6</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>1361817066</td>\n",
              "      <td>-480</td>\n",
              "      <td>11d3419bb96544dfdc47a5b1586690d7ad0fdccf</td>\n",
              "      <td>remove bogus comment\\n</td>\n",
              "      <td>SPARK-15099</td>\n",
              "      <td>12964788.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>yanboliang</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-16T18:10:38.000+0000</td>\n",
              "      <td>2016-05-03 21:03:54+00:00</td>\n",
              "      <td>2016-06-16T18:10:38.000+0000</td>\n",
              "      <td>Documentation,ML</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15399</th>\n",
              "      <td>NaN</td>\n",
              "      <td>baab23abdfc94af32c35d5fc2035382d3faa0ec4</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>Imran Rashid</td>\n",
              "      <td>imran@quantifind.com</td>\n",
              "      <td>1361484781</td>\n",
              "      <td>-480</td>\n",
              "      <td>15b1b97892f39fa90b571f03769a98f34fa658fc</td>\n",
              "      <td>TaskContext does not hold a reference to Task;...</td>\n",
              "      <td>SPARK-15086</td>\n",
              "      <td>12964526.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>srowen</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-12T18:44:38.000+0000</td>\n",
              "      <td>2016-05-03 05:03:13+00:00</td>\n",
              "      <td>2016-06-12T18:44:38.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15958</th>\n",
              "      <td>NaN</td>\n",
              "      <td>f5b1fecb9fc2e7d389810bfff5298f42b313f208</td>\n",
              "      <td>Harold Lim</td>\n",
              "      <td>harold@cs.duke.edu</td>\n",
              "      <td>Andrew xia</td>\n",
              "      <td>junluan.xia@intel.com</td>\n",
              "      <td>1363066287</td>\n",
              "      <td>480</td>\n",
              "      <td>6469300e13b4fbb515c8ce72388dc509c9065928</td>\n",
              "      <td>Cleaned up the code\\n</td>\n",
              "      <td>SPARK-14526</td>\n",
              "      <td>12957618.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>davies</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Bug</td>\n",
              "      <td>davies</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-04-29T21:31:12.000+0000</td>\n",
              "      <td>2016-04-11 06:51:55+00:00</td>\n",
              "      <td>2016-04-29T21:31:12.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16014</th>\n",
              "      <td>NaN</td>\n",
              "      <td>9f84315c055d7a53da8787eb26b336726fc33e8a</td>\n",
              "      <td>Gavin Li</td>\n",
              "      <td>lyo.gavin@gmail.com</td>\n",
              "      <td>Gavin Li</td>\n",
              "      <td>lyo.gavin@gmail.com</td>\n",
              "      <td>1370046370</td>\n",
              "      <td>0</td>\n",
              "      <td>fa3800c2375fff07086e24be7629c3b398545a2b</td>\n",
              "      <td>enhance pipe to support what we can do in hado...</td>\n",
              "      <td>SPARK-17183</td>\n",
              "      <td>12998880.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>cloud_fan</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>cloud_fan</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>cloud_fan</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-11-05T07:58:27.000+0000</td>\n",
              "      <td>2016-08-22 12:54:27+00:00</td>\n",
              "      <td>2016-11-05T07:58:26.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16055</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4fd86e0e10149ad1803831a308a056c7105cbe67</td>\n",
              "      <td>Mingfei</td>\n",
              "      <td>mingfei.shi@intel.com</td>\n",
              "      <td>Mingfei</td>\n",
              "      <td>mingfei.shi@intel.com</td>\n",
              "      <td>1370677547</td>\n",
              "      <td>480</td>\n",
              "      <td>b44c320a669cf53eaa7b91937e0de619febc0c49</td>\n",
              "      <td>delete test code for joblogger in SparkContext\\n</td>\n",
              "      <td>SPARK-17142</td>\n",
              "      <td>12998323.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>jiangxb1987</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-14T05:46:06.000+0000</td>\n",
              "      <td>2016-08-18 21:53:19+00:00</td>\n",
              "      <td>2016-09-13T15:08:47.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16077</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6c8d1b2ca618a1a17566ede46821c0807a1b11f5</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>1372198860</td>\n",
              "      <td>-240</td>\n",
              "      <td>c726a4023ac97d520ed959f45239b1c932db0d5b</td>\n",
              "      <td>Fix computation of classpath when we launch ja...</td>\n",
              "      <td>SPARK-17120</td>\n",
              "      <td>12998076.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-08-31T21:14:41.000+0000</td>\n",
              "      <td>2016-08-18 00:47:50+00:00</td>\n",
              "      <td>2016-08-25T12:24:00.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16098</th>\n",
              "      <td>NaN</td>\n",
              "      <td>cfcda95f86c6eec4a39e5ad182e068722be66fe7</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>Tathagata Das</td>\n",
              "      <td>tathagata.das1565@gmail.com</td>\n",
              "      <td>1372135490</td>\n",
              "      <td>-420</td>\n",
              "      <td>55d3f7f1f1b299c44150694a1f044b93dfa28211</td>\n",
              "      <td>Merge pull request #571 from Reinvigorate/sm-k...</td>\n",
              "      <td>SPARK-17099</td>\n",
              "      <td>12997783.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-08-31T21:15:09.000+0000</td>\n",
              "      <td>2016-08-17 02:17:23+00:00</td>\n",
              "      <td>2016-08-25T12:23:25.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16625</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5133e4bebd47d8ae089f967689ecab551c2c5844</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei.zaharia@gmail.com</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei.zaharia@gmail.com</td>\n",
              "      <td>1375915845</td>\n",
              "      <td>-420</td>\n",
              "      <td>63c1f4dbbb82495faaa3ce1dbc2fe594451d6aa1</td>\n",
              "      <td>Merge pull request #790 from kayousterhout/fix...</td>\n",
              "      <td>SPARK-17673</td>\n",
              "      <td>13007762.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rspitzer</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>ekhliang</td>\n",
              "      <td>Bug</td>\n",
              "      <td>rspitzer</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-28T23:44:50.000+0000</td>\n",
              "      <td>2016-09-26 23:42:37+00:00</td>\n",
              "      <td>2016-09-28T20:23:34.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16632</th>\n",
              "      <td>SPARK-826</td>\n",
              "      <td>cb6be5bd7eab8b3cf682a6d0347b87d216d43939</td>\n",
              "      <td>Dmitriy Lyubimov</td>\n",
              "      <td>dlyubimov@apache.org</td>\n",
              "      <td>Dmitriy Lyubimov</td>\n",
              "      <td>dlyubimov@apache.org</td>\n",
              "      <td>1375333762</td>\n",
              "      <td>-420</td>\n",
              "      <td>5071c649b24b37e5669571981da8100aaec3aaf8</td>\n",
              "      <td>Merge remote-tracking branch 'mesos/master' in...</td>\n",
              "      <td>SPARK-17666</td>\n",
              "      <td>13007595.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>igor.berman</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>igor.berman</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-30T02:07:24.000+0000</td>\n",
              "      <td>2016-09-26 14:44:15+00:00</td>\n",
              "      <td>2016-09-28T01:14:24.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16671</th>\n",
              "      <td>NaN</td>\n",
              "      <td>cce758b8938afb24c6d61a02f697201c41801fb6</td>\n",
              "      <td>Nick Pentreath</td>\n",
              "      <td>nick.pentreath@gmail.com</td>\n",
              "      <td>Nick Pentreath</td>\n",
              "      <td>nick.pentreath@gmail.com</td>\n",
              "      <td>1375886332</td>\n",
              "      <td>120</td>\n",
              "      <td>76590ceb47fbc22ddc431fd3e20817769ecc11c0</td>\n",
              "      <td>Adding Scala version of PageRank example\\n</td>\n",
              "      <td>SPARK-17627</td>\n",
              "      <td>13006682.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>Bug</td>\n",
              "      <td>marmbrus</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-22T03:59:18.000+0000</td>\n",
              "      <td>2016-09-21 22:32:40+00:00</td>\n",
              "      <td>2016-09-22T03:59:18.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16680</th>\n",
              "      <td>NaN</td>\n",
              "      <td>81e1d4a7d19cefe4bbf0fe6ec54b4f894d7b21d0</td>\n",
              "      <td>Kay Ousterhout</td>\n",
              "      <td>kayo@yahoo-inc.com</td>\n",
              "      <td>Kay Ousterhout</td>\n",
              "      <td>kayo@yahoo-inc.com</td>\n",
              "      <td>1376080061</td>\n",
              "      <td>-420</td>\n",
              "      <td>e147854e0d30e5a84f2e5d8a164fc7e8075e0d40</td>\n",
              "      <td>Refactored SparkListener to process all events...</td>\n",
              "      <td>SPARK-17618</td>\n",
              "      <td>13006362.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>gedwards</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>Bug</td>\n",
              "      <td>gedwards</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-09-28T23:27:26.000+0000</td>\n",
              "      <td>2016-09-21 02:51:12+00:00</td>\n",
              "      <td>2016-09-27T18:01:52.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17012</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5a18b854a704fc37dc268f9183552da8655d5b1d</td>\n",
              "      <td>Evan Chan</td>\n",
              "      <td>ev@ooyala.com</td>\n",
              "      <td>Evan Chan</td>\n",
              "      <td>ev@ooyala.com</td>\n",
              "      <td>1378501183</td>\n",
              "      <td>-420</td>\n",
              "      <td>dfef4b915734058adb2798c1051371eb0780bda8</td>\n",
              "      <td>Easier way to start the master\\n</td>\n",
              "      <td>SPARK-14673</td>\n",
              "      <td>12959304.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Bug</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-04-29T23:57:30.000+0000</td>\n",
              "      <td>2016-04-15 22:14:31+00:00</td>\n",
              "      <td>2016-04-29T23:57:30.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17043</th>\n",
              "      <td>NaN</td>\n",
              "      <td>c0d375107f414822d65eaff0e3a76dd3fe9e1570</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>1378626281</td>\n",
              "      <td>-420</td>\n",
              "      <td>087c293ae71305f25303893a5d8863d1c7878e0d</td>\n",
              "      <td>Some tweaks to CDH/HDP doc\\n</td>\n",
              "      <td>SPARK-14642</td>\n",
              "      <td>12958974.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>sbcd90</td>\n",
              "      <td>Bug</td>\n",
              "      <td>yhuai</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-10T19:35:40.000+0000</td>\n",
              "      <td>2016-04-14 21:31:15+00:00</td>\n",
              "      <td>2016-05-10T19:34:38.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17070</th>\n",
              "      <td>NaN</td>\n",
              "      <td>3f283278b00fc0a98a6c8cccd704bfc476f5d765</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1380229090</td>\n",
              "      <td>-420</td>\n",
              "      <td>3f5d85e375b999998cf7bf3f351090e0822d31ca</td>\n",
              "      <td>Removed scala -optimize flag.\\n</td>\n",
              "      <td>SPARK-14615</td>\n",
              "      <td>12958654.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>dbtsai</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-21T08:05:18.000+0000</td>\n",
              "      <td>2016-04-13 23:10:11+00:00</td>\n",
              "      <td>2016-05-17T19:52:14.000+0000</td>\n",
              "      <td>Build,ML</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17202</th>\n",
              "      <td>NaN</td>\n",
              "      <td>466fd06475d8ed262c456421ed2dceba54229db1</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1385375246</td>\n",
              "      <td>480</td>\n",
              "      <td>f61f77c6f9488c27539c08b71035cb6483951ff2</td>\n",
              "      <td>Incorporated ideas from pull request #200.\\n- ...</td>\n",
              "      <td>SPARK-15490</td>\n",
              "      <td>12972170.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>vectorijk</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-27T08:26:14.000+0000</td>\n",
              "      <td>2016-05-23 20:06:36+00:00</td>\n",
              "      <td>2016-06-17T02:39:55.000+0000</td>\n",
              "      <td>SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17235</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1e9543b567b81cf3207984402269d230c10e713e</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1383459541</td>\n",
              "      <td>-420</td>\n",
              "      <td>326ea5a89f1b83949350baae7b209c7b7d4b779a</td>\n",
              "      <td>Fixed a bug that uses twice amount of memory f...</td>\n",
              "      <td>SPARK-15457</td>\n",
              "      <td>12971699.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>josephkb</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-15T20:45:14.000+0000</td>\n",
              "      <td>2016-05-20 22:55:04+00:00</td>\n",
              "      <td>2016-06-15T20:45:14.000+0000</td>\n",
              "      <td>ML,MLlib</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17247</th>\n",
              "      <td>NaN</td>\n",
              "      <td>e2047d3927e0032cc1d6de3fd09d00f96ce837d0</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1381521885</td>\n",
              "      <td>-420</td>\n",
              "      <td>e55fdb2ad60e5a57ad586b4781e8323a271946e6</td>\n",
              "      <td>Making takeAsync and collectAsync deterministi...</td>\n",
              "      <td>SPARK-15445</td>\n",
              "      <td>12971475.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>techaddict</td>\n",
              "      <td>Bug</td>\n",
              "      <td>m1lan</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-23T08:01:42.000+0000</td>\n",
              "      <td>2016-05-20 09:31:55+00:00</td>\n",
              "      <td>2016-05-21T11:40:09.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17248</th>\n",
              "      <td>NaN</td>\n",
              "      <td>09f7609254a8b70a551e7403bc5378434318b3f4</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1381515615</td>\n",
              "      <td>-420</td>\n",
              "      <td>0bc58e0a2856da2c581874686135b7ab7f0a3944</td>\n",
              "      <td>Properly handle interrupted exception in Futur...</td>\n",
              "      <td>SPARK-15444</td>\n",
              "      <td>12971471.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>viirya</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>viirya</td>\n",
              "      <td>Test</td>\n",
              "      <td>viirya</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-20T20:27:40.000+0000</td>\n",
              "      <td>2016-05-20 09:15:23+00:00</td>\n",
              "      <td>2016-05-20T11:40:50.000+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17255</th>\n",
              "      <td>NaN</td>\n",
              "      <td>357733d292230968c6bda68dbe9407c560d97c91</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1381436858</td>\n",
              "      <td>-420</td>\n",
              "      <td>4ff7800bf9d96fb6dfcf3165454b029d73426df3</td>\n",
              "      <td>Rename kill -&gt; cancel in user facing API / doc...</td>\n",
              "      <td>SPARK-15437</td>\n",
              "      <td>12971420.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Bug</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-21T05:02:04.000+0000</td>\n",
              "      <td>2016-05-20 06:18:45+00:00</td>\n",
              "      <td>2016-05-21T05:02:04.000+0000</td>\n",
              "      <td>SparkR</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17266</th>\n",
              "      <td>NaN</td>\n",
              "      <td>c5e40954eb9e8f46f62542f42ff9bb95cc708447</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>reynoldx@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>reynoldx@gmail.com</td>\n",
              "      <td>1379641478</td>\n",
              "      <td>-420</td>\n",
              "      <td>07a6074b2836699dd12a7e9dd33a8c8cc704da5f</td>\n",
              "      <td>Wrap around cached data to InterruptibleIterat...</td>\n",
              "      <td>SPARK-15426</td>\n",
              "      <td>12971369.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>rxin</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-06-15T16:44:03.000+0000</td>\n",
              "      <td>2016-05-20 01:13:29+00:00</td>\n",
              "      <td>2016-06-15T16:44:03.000+0000</td>\n",
              "      <td>SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17275</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1cb42e6b2d3380f9d9a78b5c7e5959a47e0309ff</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>reynoldx@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>reynoldx@gmail.com</td>\n",
              "      <td>1379394645</td>\n",
              "      <td>-420</td>\n",
              "      <td>308e61481d1465114a9eba5cdbac7601047be00d</td>\n",
              "      <td>Properly handle job failure when the job gets ...</td>\n",
              "      <td>SPARK-15417</td>\n",
              "      <td>12971293.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>andrewor14</td>\n",
              "      <td>Bug</td>\n",
              "      <td>smilegator</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-20T18:16:53.000+0000</td>\n",
              "      <td>2016-05-19 20:35:04+00:00</td>\n",
              "      <td>2016-05-20T06:44:31.000+0000</td>\n",
              "      <td>PySpark,SQL</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17289</th>\n",
              "      <td>NaN</td>\n",
              "      <td>d6d11c2edbd11d2fde6dceb706711f2a4c3cf39d</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>Matei Zaharia</td>\n",
              "      <td>matei@eecs.berkeley.edu</td>\n",
              "      <td>1383345633</td>\n",
              "      <td>-420</td>\n",
              "      <td>1e666ae4b51b1a7cceb0e6ccf3dfe278480c4086</td>\n",
              "      <td>Merge pull request #129 from velvia/2013-11/do...</td>\n",
              "      <td>SPARK-15403</td>\n",
              "      <td>12971102.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>rain_dev</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Bug</td>\n",
              "      <td>rain_dev</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-05-20T11:16:08.000+0000</td>\n",
              "      <td>2016-05-19 10:22:28+00:00</td>\n",
              "      <td>2016-05-20T11:16:08.000+0000</td>\n",
              "      <td>MLlib</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               key  ... severity\n",
              "322    SPARK-14525  ...     10.0\n",
              "391            NaN  ...     10.0\n",
              "618    SPARK-17356  ...     10.0\n",
              "626            NaN  ...     10.0\n",
              "706    SPARK-17301  ...     10.0\n",
              "746    SPARK-12978  ...     10.0\n",
              "796    SPARK-17115  ...     10.0\n",
              "823    SPARK-17136  ...     10.0\n",
              "896    SPARK-16941  ...     10.0\n",
              "1511   SPARK-16053  ...     10.0\n",
              "1591   SPARK-15782  ...     10.0\n",
              "1772   SPARK-15092  ...     10.0\n",
              "1780   SPARK-15076  ...     10.0\n",
              "2808   SPARK-14470  ...     10.0\n",
              "2830   SPARK-12610  ...     10.0\n",
              "3273   SPARK-13658  ...     10.0\n",
              "3523   SPARK-13292  ...     10.0\n",
              "3539   SPARK-13482  ...     10.0\n",
              "3575   SPARK-13236  ...     10.0\n",
              "5026   SPARK-11469  ...     10.0\n",
              "6075    SPARK-9903  ...     10.0\n",
              "6110    SPARK-9806  ...     10.0\n",
              "6804    SPARK-8221  ...     10.0\n",
              "7006           NaN  ...     10.0\n",
              "7733    SPARK-7788  ...     10.0\n",
              "7777    SPARK-7752  ...     10.0\n",
              "8507    SPARK-6207  ...     10.0\n",
              "8556    SPARK-3074  ...     10.0\n",
              "8580    SPARK-6705  ...     10.0\n",
              "8593    SPARK-4313  ...     10.0\n",
              "...            ...  ...      ...\n",
              "14796          NaN  ...     10.0\n",
              "15308          NaN  ...     10.0\n",
              "15320          NaN  ...     10.0\n",
              "15326          NaN  ...     10.0\n",
              "15356          NaN  ...     10.0\n",
              "15361          NaN  ...     10.0\n",
              "15373          NaN  ...     10.0\n",
              "15385          NaN  ...     10.0\n",
              "15386          NaN  ...     10.0\n",
              "15399          NaN  ...     10.0\n",
              "15958          NaN  ...     10.0\n",
              "16014          NaN  ...     10.0\n",
              "16055          NaN  ...     10.0\n",
              "16077          NaN  ...     10.0\n",
              "16098          NaN  ...     10.0\n",
              "16625          NaN  ...     10.0\n",
              "16632    SPARK-826  ...     10.0\n",
              "16671          NaN  ...     10.0\n",
              "16680          NaN  ...     10.0\n",
              "17012          NaN  ...     10.0\n",
              "17043          NaN  ...     10.0\n",
              "17070          NaN  ...     10.0\n",
              "17202          NaN  ...     10.0\n",
              "17235          NaN  ...     10.0\n",
              "17247          NaN  ...     10.0\n",
              "17248          NaN  ...     10.0\n",
              "17255          NaN  ...     10.0\n",
              "17266          NaN  ...     10.0\n",
              "17275          NaN  ...     10.0\n",
              "17289          NaN  ...     10.0\n",
              "\n",
              "[82 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q90L8-7Th4f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['updated'] = pd.to_datetime(df['updated'], errors='coerce', utc='Europe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkAYjBRPu-Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['commit duration'] = df['updated'] - df['created']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skisOsE2diWy",
        "colab_type": "code",
        "outputId": "90dda91f-22b0-4c85-9323-283767aac603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "df.sort_values(['commit duration'], ascending=False).iloc[:10,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>cid</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_email</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_email</th>\n",
              "      <th>time</th>\n",
              "      <th>time_offset</th>\n",
              "      <th>tree_id</th>\n",
              "      <th>message_encoding</th>\n",
              "      <th>key</th>\n",
              "      <th>jira_id</th>\n",
              "      <th>status_category</th>\n",
              "      <th>creator</th>\n",
              "      <th>priority</th>\n",
              "      <th>status</th>\n",
              "      <th>assignee</th>\n",
              "      <th>issuetype</th>\n",
              "      <th>reporter</th>\n",
              "      <th>resolution</th>\n",
              "      <th>project</th>\n",
              "      <th>updated</th>\n",
              "      <th>created</th>\n",
              "      <th>resolved</th>\n",
              "      <th>components</th>\n",
              "      <th>severity</th>\n",
              "      <th>commit duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7219</th>\n",
              "      <td>SPARK-8138</td>\n",
              "      <td>cc465fd92482737c21971d82e30d4cf247acf932</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>1435137432</td>\n",
              "      <td>-420</td>\n",
              "      <td>f95d331889694cb87836c5a99b21e9072e38566e</td>\n",
              "      <td>[SPARK-8138] [SQL] Improves error message when...</td>\n",
              "      <td>SPARK-636</td>\n",
              "      <td>12705446.0</td>\n",
              "      <td>New</td>\n",
              "      <td>None</td>\n",
              "      <td>Major</td>\n",
              "      <td>Open</td>\n",
              "      <td>None</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-15 15:18:02+00:00</td>\n",
              "      <td>2012-12-13 09:45:59+00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1402 days 05:32:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7205</th>\n",
              "      <td>SPARK-7884</td>\n",
              "      <td>7bac2fe7717c0102b4875dbd95ae0bbf964536e3</td>\n",
              "      <td>Matt Massie</td>\n",
              "      <td>massie@cs.berkeley.edu</td>\n",
              "      <td>Kay Ousterhout</td>\n",
              "      <td>kayousterhout@gmail.com</td>\n",
              "      <td>1435209006</td>\n",
              "      <td>-420</td>\n",
              "      <td>fcc3f32bc349b48411eee2e5073eae346e0718ea</td>\n",
              "      <td>[SPARK-7884] Move block deserialization from B...</td>\n",
              "      <td>SPARK-650</td>\n",
              "      <td>12705195.0</td>\n",
              "      <td>New</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Reopened</td>\n",
              "      <td>None</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>matei</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-17 11:39:41+00:00</td>\n",
              "      <td>2013-01-11 16:46:50+00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1374 days 18:52:51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7262</th>\n",
              "      <td>SPARK-7426</td>\n",
              "      <td>47c1d5629373566df9d12fdc4ceb22f38b869482</td>\n",
              "      <td>Mike Dusenberry</td>\n",
              "      <td>dusenberrymw@gmail.com</td>\n",
              "      <td>Joseph K. Bradley</td>\n",
              "      <td>joseph@databricks.com</td>\n",
              "      <td>1434936336</td>\n",
              "      <td>-420</td>\n",
              "      <td>34885825c9706d48e4a943c30ac1ee52aa529b94</td>\n",
              "      <td>[SPARK-7426] [MLLIB] [ML] Updated Attribute.fr...</td>\n",
              "      <td>SPARK-593</td>\n",
              "      <td>12705246.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>dennybritz</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>dennybritz</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-01-16 13:00:13+00:00</td>\n",
              "      <td>2012-10-24 15:58:43+00:00</td>\n",
              "      <td>2016-01-16T13:00:13.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1178 days 21:01:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7260</th>\n",
              "      <td>SPARK-8513</td>\n",
              "      <td>0818fdec3733ec5c0a9caa48a9c0f2cd25f84d13</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian@databricks.com</td>\n",
              "      <td>Yin Huai</td>\n",
              "      <td>yhuai@databricks.com</td>\n",
              "      <td>1434992637</td>\n",
              "      <td>-420</td>\n",
              "      <td>a36710eceeac3eec994270cde2a9ad8f1a55fa56</td>\n",
              "      <td>[SPARK-8406] [SQL] Adding UUID to output file ...</td>\n",
              "      <td>SPARK-595</td>\n",
              "      <td>12705088.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-01-13 10:23:54+00:00</td>\n",
              "      <td>2012-10-26 15:26:45+00:00</td>\n",
              "      <td>2016-01-13T10:23:53.000+0000</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1173 days 18:57:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7271</th>\n",
              "      <td>SPARK-8422</td>\n",
              "      <td>7a3c424ecf815b9d5e06e222dd875e5a31a26400</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>Davies Liu</td>\n",
              "      <td>davies@databricks.com</td>\n",
              "      <td>1434841554</td>\n",
              "      <td>-420</td>\n",
              "      <td>3a4e6bb417366e80c53578e94a6fd8fa9b77f607</td>\n",
              "      <td>[SPARK-8422] [BUILD] [PROJECT INFRA] Add a mod...</td>\n",
              "      <td>SPARK-584</td>\n",
              "      <td>12704937.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>None</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-01-05 21:13:57+00:00</td>\n",
              "      <td>2012-10-22 14:39:08+00:00</td>\n",
              "      <td>2016-01-05T21:13:56.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1170 days 06:34:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16174</th>\n",
              "      <td>NaN</td>\n",
              "      <td>32a45d01b104f79d9e5f10c333c44cbf910949ef</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>1371922296</td>\n",
              "      <td>-420</td>\n",
              "      <td>0de7b6c42c61ae32cada012d0c7de48a499ba540</td>\n",
              "      <td>Removing twirl files\\n</td>\n",
              "      <td>SPARK-882</td>\n",
              "      <td>12705018.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>patrick</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>pwendell</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-31 09:38:42+00:00</td>\n",
              "      <td>2013-08-21 00:40:50+00:00</td>\n",
              "      <td>2016-10-31T09:38:40.000+0000</td>\n",
              "      <td>Documentation</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1167 days 08:57:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7221</th>\n",
              "      <td>NaN</td>\n",
              "      <td>13ae806b255cfb2bd5470b599a95c87a2cd5e978</td>\n",
              "      <td>Josh Rosen</td>\n",
              "      <td>joshrosen@databricks.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@databricks.com</td>\n",
              "      <td>1435125839</td>\n",
              "      <td>-420</td>\n",
              "      <td>561905e27af2ab6c33235f2d6a5d0d99fae0ab08</td>\n",
              "      <td>[HOTFIX] [BUILD] Fix MiMa checks in master bra...</td>\n",
              "      <td>SPARK-634</td>\n",
              "      <td>12705111.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Major</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>patrick</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>rxin</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-01-18 10:22:46+00:00</td>\n",
              "      <td>2012-12-11 11:38:55+00:00</td>\n",
              "      <td>2016-01-18T10:22:46.000+0000</td>\n",
              "      <td>Block Manager</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1132 days 22:43:51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16128</th>\n",
              "      <td>NaN</td>\n",
              "      <td>37abe84212c6dad6fb87a3b47666d6a3c14c1f66</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>Patrick Wendell</td>\n",
              "      <td>pwendell@gmail.com</td>\n",
              "      <td>1373127599</td>\n",
              "      <td>-420</td>\n",
              "      <td>44a52557cd0efa9478ad0872209dfdd76c993bae</td>\n",
              "      <td>Tracking some task metrics even during failure...</td>\n",
              "      <td>SPARK-928</td>\n",
              "      <td>12705453.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>techaddict</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>matei</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-10-22 19:03:12+00:00</td>\n",
              "      <td>2013-10-09 16:59:39+00:00</td>\n",
              "      <td>2016-10-22T19:03:12.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1109 days 02:03:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12123</th>\n",
              "      <td>SPARK-1959</td>\n",
              "      <td>cf989601d0e784e1c3507720e64636891fe28292</td>\n",
              "      <td>Cheng Lian</td>\n",
              "      <td>lian.cs.zju@gmail.com</td>\n",
              "      <td>Reynold Xin</td>\n",
              "      <td>rxin@apache.org</td>\n",
              "      <td>1401513191</td>\n",
              "      <td>-420</td>\n",
              "      <td>f0ff5600ebe356a0cc66b475a43a97d5889a75d7</td>\n",
              "      <td>[SPARK-1959] String \"NULL\" shouldn't be interp...</td>\n",
              "      <td>SPARK-732</td>\n",
              "      <td>12704822.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>Closed</td>\n",
              "      <td>CodingCat</td>\n",
              "      <td>Bug</td>\n",
              "      <td>joshrosen</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2016-02-24 22:23:34+00:00</td>\n",
              "      <td>2013-04-14 22:50:16+00:00</td>\n",
              "      <td>2014-11-27T01:32:59.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1045 days 23:33:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7252</th>\n",
              "      <td>SPARK-8455</td>\n",
              "      <td>afe35f0519bc7dcb85010a7eedcff854d4fc313a</td>\n",
              "      <td>Feynman Liang</td>\n",
              "      <td>fliang@databricks.com</td>\n",
              "      <td>Joseph K. Bradley</td>\n",
              "      <td>joseph@databricks.com</td>\n",
              "      <td>1435007735</td>\n",
              "      <td>-420</td>\n",
              "      <td>d123b5a16e88c8a5a3762df08bdfd450c5802bcf</td>\n",
              "      <td>[SPARK-8455] [ML] Implement n-gram feature tra...</td>\n",
              "      <td>SPARK-603</td>\n",
              "      <td>12705451.0</td>\n",
              "      <td>Complete</td>\n",
              "      <td>None</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Closed</td>\n",
              "      <td>None</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>imranr</td>\n",
              "      <td>None</td>\n",
              "      <td>Spark</td>\n",
              "      <td>2015-09-09 04:06:27+00:00</td>\n",
              "      <td>2012-11-01 12:46:28+00:00</td>\n",
              "      <td>2015-03-03T15:30:51.000+0000</td>\n",
              "      <td>Spark Core</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1041 days 15:19:59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              key  ...    commit duration\n",
              "7219   SPARK-8138  ... 1402 days 05:32:03\n",
              "7205   SPARK-7884  ... 1374 days 18:52:51\n",
              "7262   SPARK-7426  ... 1178 days 21:01:30\n",
              "7260   SPARK-8513  ... 1173 days 18:57:09\n",
              "7271   SPARK-8422  ... 1170 days 06:34:49\n",
              "16174         NaN  ... 1167 days 08:57:52\n",
              "7221          NaN  ... 1132 days 22:43:51\n",
              "16128         NaN  ... 1109 days 02:03:33\n",
              "12123  SPARK-1959  ... 1045 days 23:33:18\n",
              "7252   SPARK-8455  ... 1041 days 15:19:59\n",
              "\n",
              "[10 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b2jwwB826jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBPF8G8g26hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI5oKPZC26d4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaB1Z6du26bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjt45hbi26ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__kMBEow26XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPi8hVQs26Uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmMr8E8j26S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1IkRff226Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiHwbseO26OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prjKv5Yb26LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8hQ6CcaiREY",
        "colab_type": "code",
        "outputId": "4f31542d-0921-41d9-febd-8d1076c932dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = df[['message_encoding', 'severity']].dropna()\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message_encoding</th>\n",
              "      <th>severity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[SPARK-16904][SQL] Removal of Hive Built-in Ha...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[SPARK-18296][SQL] Use consistent naming for e...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[SPARK-18167][SQL] Disable flaky hive partitio...</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[SPARK-18173][SQL] data source tables should s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[SPARK-18269][SQL] CSV datasource should read ...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    message_encoding  severity\n",
              "0  [SPARK-16904][SQL] Removal of Hive Built-in Ha...       3.0\n",
              "1  [SPARK-18296][SQL] Use consistent naming for e...       1.0\n",
              "2  [SPARK-18167][SQL] Disable flaky hive partitio...      10.0\n",
              "3  [SPARK-18173][SQL] data source tables should s...       1.0\n",
              "4  [SPARK-18269][SQL] CSV datasource should read ...       3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itWZW8Rn2CPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['severity'] = df['severity'].astype(int).map({0:0,1:1,3:2,5:3,10:4})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQkvZfdWo0S7",
        "colab_type": "code",
        "outputId": "dd75f165-6fbb-4b3d-a5db-a509567902bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 17966 entries, 0 to 17965\n",
            "Data columns (total 2 columns):\n",
            "message_encoding    17966 non-null object\n",
            "severity            17966 non-null int64\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 421.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnwQvhkCzvfN",
        "colab_type": "code",
        "outputId": "0d22811c-ff18-46ce-f21e-86e98586d05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "df['severity'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    10388\n",
              "1     4359\n",
              "3     1218\n",
              "0     1035\n",
              "4      966\n",
              "Name: severity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239AbhrliRCk",
        "colab_type": "code",
        "outputId": "5d1075cc-5708-438f-b7a4-66177674e172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "print(df['message_encoding'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SPARK-16904][SQL] Removal of Hive Built-in Hash Functions and TestHiveFunctionRegistry\n",
            "\n",
            "### What changes were proposed in this pull request?\n",
            "\n",
            "Currently, the Hive built-in `hash` function is not being used in Spark since Spark 2.0. The public interface does not allow users to unregister the Spark built-in functions. Thus, users will never use Hive's built-in `hash` function.\n",
            "\n",
            "The only exception here is `TestHiveFunctionRegistry`, which allows users to unregister the built-in functions. Thus, we can load Hive's hash function in the test cases. If we disable it, 10+ test cases will fail because the results are different from the Hive golden answer files.\n",
            "\n",
            "This PR is to remove `hash` from the list of `hiveFunctions` in `HiveSessionCatalog`. It will also remove `TestHiveFunctionRegistry`. This removal makes us easier to remove `TestHiveSessionState` in the future.\n",
            "### How was this patch tested?\n",
            "N/A\n",
            "\n",
            "Author: gatorsmile <gatorsmile@gmail.com>\n",
            "\n",
            "Closes #14498 from gatorsmile/removeHash.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tjon3J4iQ_z",
        "colab_type": "code",
        "outputId": "2df79eee-b9bc-46b3-b3c4-c209bf197ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df['message_encoding'][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[SPARK-18296][SQL] Use consistent naming for expression test suites\\n\\n## What changes were proposed in this pull request?\\nWe have an undocumented naming convention to call expression unit tests ExpressionsSuite, and the end-to-end tests FunctionsSuite. It'd be great to make all test suites consistent with this naming convention.\\n\\n## How was this patch tested?\\nThis is a test-only naming change.\\n\\nAuthor: Reynold Xin <rxin@databricks.com>\\n\\nCloses #15793 from rxin/SPARK-18296.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gVEKxvwiQ9p",
        "colab_type": "code",
        "outputId": "6e46e2e1-3cef-4bd5-b848-0682b2cebcf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "print(df['message_encoding'][1], df['severity'][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SPARK-18296][SQL] Use consistent naming for expression test suites\n",
            "\n",
            "## What changes were proposed in this pull request?\n",
            "We have an undocumented naming convention to call expression unit tests ExpressionsSuite, and the end-to-end tests FunctionsSuite. It'd be great to make all test suites consistent with this naming convention.\n",
            "\n",
            "## How was this patch tested?\n",
            "This is a test-only naming change.\n",
            "\n",
            "Author: Reynold Xin <rxin@databricks.com>\n",
            "\n",
            "Closes #15793 from rxin/SPARK-18296.\n",
            " 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuQNHA8GiQ6X",
        "colab_type": "code",
        "outputId": "76d92ed0-427e-43a9-9e15-f39aaacab816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "print(df['message_encoding'][3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SPARK-18173][SQL] data source tables should support truncating partition\n",
            "\n",
            "## What changes were proposed in this pull request?\n",
            "\n",
            "Previously `TRUNCATE TABLE ... PARTITION` will always truncate the whole table for data source tables, this PR fixes it and improve `InMemoryCatalog` to make this command work with it.\n",
            "## How was this patch tested?\n",
            "\n",
            "existing tests\n",
            "\n",
            "Author: Wenchen Fan <wenchen@databricks.com>\n",
            "\n",
            "Closes #15688 from cloud-fan/truncate.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1LLLg2QiQ3s",
        "colab_type": "code",
        "outputId": "cccabbea-be8a-4d71-c89e-eccb15ede5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "nltk.download('english')\n",
        "nltk.download('stopwords')\n",
        "stopwords.words(\"english\")\n",
        "def message_to_words( raw_message ):\n",
        "    '''\n",
        "    Function to convert a raw mesage to a string of words\n",
        "    The input is a single string (a raw message), and \n",
        "    the output is a single string (a preprocessed message)'''\n",
        "    \n",
        "    raw_message = str(raw_message.split('\\n')[2:-5])\n",
        "    \n",
        "    # 2. Remove non-letters        \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_message) \n",
        "    \n",
        "    # 3. Convert to lower case, split into individual words\n",
        "    words = letters_only.lower().split()                             \n",
        "    \n",
        "    # 4. In Python, searching a set is much faster than searching\n",
        "    #   a list, so convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    \n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]   \n",
        "\n",
        "    # 6. Join the words back into one string separated by space, \n",
        "    # and return the result.\n",
        "    return( \" \".join( meaningful_words ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading english: Package 'english' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks4fQoV6mJgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# Initialize an empty list to hold the clean messages\n",
        "clean_train_message = []\n",
        "\n",
        "for message in df['message_encoding']:\n",
        "  clean_train_message.append(message_to_words(message))\n",
        "  \n",
        "# print( \"Creating the bag of words...\\n\")\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
        "#                              tokenizer = None,    \\\n",
        "#                              preprocessor = None, \\\n",
        "#                              stop_words = None,   \\\n",
        "#                              max_features = 5000)\n",
        "\n",
        "# train_data_features = vectorizer.fit_transform(clean_train_message)\n",
        "# train_data_features = train_data_features.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PII6bzXVvgTj",
        "colab_type": "code",
        "outputId": "eb40fba7-371c-4d93-f4ac-b0ef798d6c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "data_mean = tfidf_vectorizer(w2v).fit(clean_train_message).transform(clean_train_message)\n",
        "\n",
        "rand_forest = RandomForestClassifier()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_mean, df[\"severity\"])\n",
        "rand_forest.fit(X_train, y_train)\n",
        "y_pred = rand_forest.predict(X_test)\n",
        "print(f1_score(y_test, y_pred, average='weighted'))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45021468000551795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  10,   36,  223,    1,    1],\n",
              "       [  12,  154,  931,   13,    7],\n",
              "       [  25,  311, 2216,   19,    8],\n",
              "       [   2,   42,  245,    4,    2],\n",
              "       [   0,   30,  194,    3,    3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMLCxXkiGnGC",
        "colab_type": "code",
        "outputId": "3b85c8cf-6a7b-49d4-bb59-c54ac2cf5c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "clean_train_message"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spark sql removal hive built hash functions testhivefunctionregistry',\n",
              " 'spark sql use consistent naming expression test suites',\n",
              " 'spark sql disable flaky hive partition pruning test',\n",
              " 'spark sql data source tables support truncating partition',\n",
              " 'spark sql csv datasource read null properly schema lager parsed tokens',\n",
              " 'spark ml pipeline copy create instance uid',\n",
              " 'spark sql rand randn allows null long input seed',\n",
              " 'spark ml ml models copy training summary set parent',\n",
              " 'minor documentation fix minor descriptions functions consistently expressions',\n",
              " 'spark sparkr enable sparkr mesos client mode cluster mode',\n",
              " 'spark sql fix npe problem using grouping sets',\n",
              " 'spark minor followup missed json test filestreamsinksuite',\n",
              " 'spark sql move hash expressions misc scala hash scala',\n",
              " 'spark spark spark sql put hive serde table schema table properties like data source table',\n",
              " 'spark spark spark sql put hive serde table schema table properties like data source table',\n",
              " 'spark spark spark sql put hive serde table schema table properties like data source table',\n",
              " 'spark make json null safe',\n",
              " 'spark follow add comments state utils classforname used',\n",
              " 'spark sql followup move test replsuite prevent java lang classcircularityerror',\n",
              " 'spark improve performance event log replay historyserver',\n",
              " 'spark enable non flaky parts sqlquerysuite',\n",
              " 'spark sql pushdown predicates filters predicate subqueries',\n",
              " 'spark core optimise appendonlymap implementation',\n",
              " 'closing stale invalid pull requests',\n",
              " 'spark graphx follow support zero initial capacity openhashset',\n",
              " 'spark sql doc update doc python r',\n",
              " 'spark sql capture throwable queryexecution',\n",
              " 'spark docs document java python scala hadoop deprecated spark',\n",
              " 'spark ss improve error reporting filestresssuite',\n",
              " 'sparkr test remove unnecessary suppresswarnings',\n",
              " 'spark ss kafka increase executor poll timeout',\n",
              " 'spark yarn fail files added distributed cache files archives',\n",
              " 'spark yarn fail files added distributed cache files archives',\n",
              " 'spark hive hive exec stagingdir effect',\n",
              " 'spark sql rename partitionproviderishive trackspartitionsincatalog',\n",
              " 'spark sql jvm object based aggregate operator',\n",
              " 'spark spark sql fix incorrect nullability setting false filterexec',\n",
              " 'spark spark sql fix incorrect nullability setting false filterexec',\n",
              " 'spark ml pyspark add missing subsamplingrate pyspark gbtclassifier',\n",
              " 'sql minor internal doc improvement insertintotable',\n",
              " 'spark move commit protocol api internal sql core core module',\n",
              " 'spark sql support drop current database',\n",
              " 'spark graphx support zero initial capacity openhashset',\n",
              " 'spark sql improve test case coverage implicit type casting',\n",
              " 'spark sql documentation add examples extend expression improve documentation',\n",
              " 'spark sql unify path data source table locationuri hive serde table',\n",
              " 'spark sql unify path data source table locationuri hive serde table',\n",
              " 'spark sql simplify runtimereplaceable type coercion',\n",
              " 'spark build add maven snapshots staging profile build test staging artifacts',\n",
              " 'spark core yarn spark files spark jars passed driver yarn mode',\n",
              " 'spark sql values generated non deterministic functions change coalesce union',\n",
              " 'spark improve doc rangebetween rowsbetween',\n",
              " 'spark sql support arraytype literal apply',\n",
              " 'spark sql simplify struct creation code path',\n",
              " 'spark core sql fix default locale used dateformat numberformat locale us',\n",
              " 'spark webui remove sparkui appuiaddress',\n",
              " 'spark doc streaming highlight code snippets',\n",
              " 'minor use clarity pi examples monte carlo process',\n",
              " 'spark add lock debugging info thread dumps',\n",
              " 'spark sql logging streamingquerylistener querystartedevent',\n",
              " 'spark support file formats structured streaming',\n",
              " 'spark spark fix insert overwrite table partition datasource tables',\n",
              " 'spark spark fix insert overwrite table partition datasource tables',\n",
              " 'spark streaming delete crc files filesystem use checksum files',\n",
              " 'spark sql return partitions hiveshim hive throws metastore exception attempting fetch partitions filter',\n",
              " 'spark sparkr check named arguments options use formatted r friendly message jvm exception message',\n",
              " 'spark sql make column expr public',\n",
              " 'spark use commit protocol api structured streaming',\n",
              " 'spark ml various chisqselector cleanups',\n",
              " 'spark expose replaylistenerbus read overload takes string iterator',\n",
              " 'spark sql disable default use kryoserializer thrift server',\n",
              " 'spark sql disable default use kryoserializer thrift server',\n",
              " 'spark sql add json supporting convert nested struct column json string',\n",
              " 'spark disable flaky sqlquerysuite test',\n",
              " 'spark sql misleading error message aggregation without window groupby',\n",
              " 'spark sql fix serialization issue keyvaluegroupeddataset',\n",
              " 'spark follow sql minor rename metadatalogfilecatalog metadatalogfileindex',\n",
              " 'spark ml move labelcol datatype cast predictor fit',\n",
              " 'revert spark sql redundant aliases cleanupaliases',\n",
              " 'spark sql redundant aliases cleanupaliases',\n",
              " 'spark sql fix unqualified catalog getfunction',\n",
              " 'spark mesos fix mesos cluster scheduler generage command option error',\n",
              " 'spark mesos migrate mesos configs use configentry',\n",
              " 'spark mesos allow enabling mesos fetch cache coarse executor backend',\n",
              " 'spark sql wrong approximatepercentile answer multiple records minimum value',\n",
              " 'minor doc remove spaces following slashs',\n",
              " 'spark sql insert overwrite statement runs much slower spark sql hive client',\n",
              " 'spark sql introduce internal commit protocol api',\n",
              " 'spark sql retry sqlquerysuite test flakes',\n",
              " 'spark sql optimize insert require repair table',\n",
              " 'spark sql also log partitions sqlquerysuite test flakes',\n",
              " 'spark tests fix flaky filestreamsourcesuite deleting files',\n",
              " 'spark sql add dataset checkpoint truncate large query plans',\n",
              " 'build close stale pull requests',\n",
              " 'spark sql ignore structured streaming event logs avoid breaking history server',\n",
              " 'spark python ml add missing parameter python randomforest regression classification',\n",
              " 'spark sparkr randomforest r',\n",
              " 'spark make timeout rbackend configurable sparkr',\n",
              " 'spark sql analyze table raise parseexception invalid option',\n",
              " 'spark sql rename filecatalog fileindex',\n",
              " 'spark sql avoid using union chain together create table repair partition commands',\n",
              " 'spark mllib kmeans clusterer return duplicate cluster centers',\n",
              " 'spark follow streaming kafka doc add java code snippet kafka integration doc',\n",
              " 'spark sql add debug code sqlquerysuite flakiness metastore partition pruning enabled',\n",
              " 'spark sql foreachsink fail spark job process throws exception',\n",
              " 'spark ml locality sensitive hashing',\n",
              " 'spark examples ml python ml pipeline example syntax e',\n",
              " 'spark ml add instrumentation gmm',\n",
              " 'spark sql unable query global temp views hive support enabled',\n",
              " 'spark sql store partition spec metastore data source table',\n",
              " 'spark sql fix test streamexecution metadata garbage collection',\n",
              " 'spark ml enhanced nan value handling bucketizer',\n",
              " 'spark sql kafka maximum data per trigger',\n",
              " 'spark core test minor fix wrong comment test',\n",
              " 'sql doc updating doc json source link jsonlines org',\n",
              " 'spark sparkr follow doc fixes',\n",
              " 'spark fix checkstyle',\n",
              " 'spark sql fix classcastexception calling tolocaliterator dataframe produced runnablecommand',\n",
              " 'spark catalyst making objecttype public',\n",
              " 'spark streaming sql changes source trait related implementation classes',\n",
              " 'spark spark core getiteratorzipwithindex accepts negative value index',\n",
              " 'spark sparkr add multiclass logistic regression sparkr wrapper',\n",
              " 'spark sql tests move group analytics test cases sqlquerysuite query file test',\n",
              " 'spark docs mllib scala mllib examples code merge clean',\n",
              " 'spark sparkr sql add storagelevel dataframe sparkr',\n",
              " 'minor ml refactor clustering summary',\n",
              " 'spark doc build kafkasource doc',\n",
              " 'spark sql failed infer constraints multiple aliases',\n",
              " 'spark sql fix concurrent executions forkjoinpool sql',\n",
              " 'spark follow ml reorg variables weightedleastsquares',\n",
              " 'spark sql fix default value test sqlconfsuite work rega',\n",
              " 'spark sql inferfiltersfromconstraints rule never terminates query',\n",
              " 'spark improved caller context logging',\n",
              " 'spark improved caller context logging',\n",
              " 'spark web ui add kill link jobs ui',\n",
              " 'spark yarn sparkstaging clean rm applicationnotfoundexception',\n",
              " 'spark sql java lang nullpointerexception instead real exception saving df mysql',\n",
              " 'spark sql fixed insert failure data source tables schema comment field',\n",
              " 'spark sparkr ml update sparkr mlp add initalweights parameter',\n",
              " 'spark spark shell spark history server log needs fixed show https url ssl enabled',\n",
              " 'spark ml add instrumentation gbts',\n",
              " 'spark ml add instrumentation gbts',\n",
              " 'spark sql binary operator consider nullability comparing input types',\n",
              " 'spark core reduce work performed building application list history server app list ui page',\n",
              " 'spark follow ml fix build error scala',\n",
              " 'spark ml followup delete superfluous line bisectingkmeans',\n",
              " 'spark sql always lowercase partition columns partition spec parser',\n",
              " 'spark ml one pass solver weighted least squares elasticnet',\n",
              " 'spark hotfix fix broken build',\n",
              " 'spark sql follow optimize query ctas',\n",
              " 'spark sql simplify tablefilecatalog',\n",
              " 'spark sql streaming test fixed flaky statestoresuite maintenance',\n",
              " 'spark core ensure uniqueness tasksetmanager name',\n",
              " 'spark sql default spark sql warehouse dir relative local fs resolve hdfs path',\n",
              " 'spark mllib test add missing tests truepositiverate weightedtruepositiverate',\n",
              " 'sparkr branch r merge api doc example fix',\n",
              " 'spark sql comparing column types ignoring nullability union setoperation',\n",
              " 'spark sql tests move hivedataframeanalyticssuite package sql',\n",
              " 'spark sql move output partitioning definition unarynodeexec children',\n",
              " 'spark sql introduce performant memory efficient apis create arraybasedmapdata',\n",
              " 'spark core add support unsafe based serializer kryo',\n",
              " 'spark core add support unsafe based serializer kryo',\n",
              " 'spark spark core fix bug custom partitioncoalescer causing serialization exception',\n",
              " 'spark sql use type widened encoder dataframe rather existing encoder allow type widening set operations',\n",
              " 'spark sql add back file status cache catalog tables',\n",
              " 'spark sql add back file status cache catalog tables',\n",
              " 'spark ml sqltransformer remove temporary tables',\n",
              " 'spark docs repositories needs username password',\n",
              " 'spark deploy sbin start scripts use hostname f fail solaris',\n",
              " 'spark minor tiny follow correct instances log message typo',\n",
              " 'spark sql outputwriter expose file path written',\n",
              " 'streaming kafka doc clarify kafka settings needed larger batches',\n",
              " 'spark sql kafka assign specific startingoffsets structured stream',\n",
              " 'spark streaming filestreamsource infer partitions every batch',\n",
              " 'spark core fix deadlock coarsegrainedschedulerbackend reset',\n",
              " 'spark sql streaming added json statuses',\n",
              " 'spark sparkr cannot parallelize data frame na null date columns',\n",
              " 'spark sparkr add crossjoin api',\n",
              " 'spark sparkr check warning test output',\n",
              " 'spark upgrade mima fix flakiness',\n",
              " 'spark web ui visually clarified executors start time timeline',\n",
              " 'spark followup ml core avoid allocating length arrays',\n",
              " 'spark pyspark upgrade py j',\n",
              " 'spark sql prunefilesourcepartitions change output logicalrelation',\n",
              " 'sparkr fix warnings',\n",
              " 'spark tests adds checks collect info filestreamsourcesuite failure',\n",
              " 'docs update docs suggest package spark running tests',\n",
              " 'spark sql refactor file name specification data sources',\n",
              " 'spark kafka sql add getpreferredlocations kafkasourcerdd',\n",
              " 'spark sql support mapvalues keyvaluegroupeddataset',\n",
              " 'spark sql join predicates contain filter clauses',\n",
              " 'spark sql show column database conflict check respect case sensitivity configuration',\n",
              " 'spark sql support wildcard character filename load data local inpath',\n",
              " 'spark deploy allow spark daemon sh run foreground',\n",
              " 'spark sql enable metastore partition pruning default',\n",
              " 'spark spark core fix bug rdd zipwithindex zipwithuniqueid index value overflowing',\n",
              " 'spark sql simplify writercontainer',\n",
              " 'spark sql check ascendingorder type sort array function rather throwing classcastexception',\n",
              " 'spark web ui allow applicationhistoryproviders provide text complete apps',\n",
              " 'spark core bump commons lang version',\n",
              " 'spark document fix broke link sparkdataframe',\n",
              " 'spark sql alter table rename allow users specify database destination table name source table',\n",
              " 'spark test hadoop fix hadoop compilation error',\n",
              " 'spark sql fix refreshbypath converted hive tables',\n",
              " 'spark sql streaming followup refactored streamingquerylistener apis',\n",
              " 'spark pyspark followup pyspark rdd repartitioning results highly skewed partition sizes',\n",
              " 'spark streaming kafka drain commitqueue',\n",
              " 'revert spark core bump commons lang version',\n",
              " 'spark sql make dataframereader jdbc call dataframereader format jdbc load',\n",
              " 'spark core serializerinstance instance used deserializing taskresult reused',\n",
              " 'minor doc add built sources sql programming guide md',\n",
              " 'spark core bump commons lang version',\n",
              " 'spark try refactor filecatalog classes simplify inheritance tree',\n",
              " 'spark compress rolled executor log',\n",
              " 'spark sql support inferring type date timestamp decimal partition column',\n",
              " 'spark sql follow debug mode work corrupted table',\n",
              " 'sql streaming test follow remove option contains scala compatibility',\n",
              " 'sql streaming test fix flaky tests streamingquerylistenersuite',\n",
              " 'revert spark refactor filecatalog classes simplify inheritance tree',\n",
              " 'spark refactor filecatalog classes simplify inheritance tree',\n",
              " 'spark sql determine serde hive default fileformat creating hive serde tables',\n",
              " 'spark sql remove spark sql eageranalysis output plan existed analysisexception',\n",
              " 'spark core use nio directbuffer instead bufferedinputstream order avoid additional copy os buffer cache user buffer',\n",
              " 'fix example tf idf mindocfreq',\n",
              " 'minor sql add prettyname current database function',\n",
              " 'spark sql add doc comment spark sql debug',\n",
              " 'spark sql support default database connection uris spark thrift server',\n",
              " 'revert spark scheduler packed scheduling spark tasks across executors',\n",
              " 'spark scheduler packed scheduling spark tasks across executors',\n",
              " 'spark documentation fix typo sparksession scaladoc',\n",
              " 'spark sql load catalog table partition metadata required answer query',\n",
              " 'spark pyspark python crossjoin api similar scala',\n",
              " 'spark sql graduate list spark sql apis stable',\n",
              " 'spark pyspark sql allow pyspark register java udf',\n",
              " 'spark sql add storagelevel dataset',\n",
              " 'spark sql add storagelevel dataset',\n",
              " 'spark sql add column distinct',\n",
              " 'revert spark sql determine serde hive default fileformat creating hive serde tables',\n",
              " 'spark sql determine serde hive default fileformat creating hive serde tables',\n",
              " 'spark ml test logistic regression tests use sample weights',\n",
              " 'test ignore flaky test streamingquerylistenersuite',\n",
              " 'typo form',\n",
              " 'doc fix typo sql hive doc',\n",
              " 'spark sql followup generate column level statistics',\n",
              " 'spark core remove query string jar url',\n",
              " 'spark mllib ml change statistic pvalue selectkbest selectpercentile dof difference',\n",
              " 'spark ml add bisectingkmeanssummary',\n",
              " 'spark ml pyspark pyspark ml evaluation support save load',\n",
              " 'spark sql metastorerelation talk external catalog instead hive client',\n",
              " 'spark sql break filesourceinterfaces scala multiple pieces',\n",
              " 'spark sql remove dead code writercontainer',\n",
              " 'spark follow ml pyspark add python api rformula forceindexlabel',\n",
              " 'spark sql add support value class serialization deserialization',\n",
              " 'spark sql consolidate various listleaffiles implementations',\n",
              " 'spark sql consolidate various listleaffiles implementations',\n",
              " 'spark sql streaming metrics structured streaming',\n",
              " 'spark sql fetch earliest offsets manually kafkasource instead counting kafkaconsumer',\n",
              " 'spark sql maxcollength type int string binary',\n",
              " 'spark sql annotate remaining sql apis interfacestability',\n",
              " 'spark sql disallow users change table type',\n",
              " 'spark core support printing scala java version spark submit version command',\n",
              " 'spark sql add debug mode keep raw table properties hiveexternalcatalog',\n",
              " 'spark web ui add support downloading event logs historyserver ui',\n",
              " 'minor doc fix row scala',\n",
              " 'spark spark sql fix dataset dropduplicates',\n",
              " 'spark spark sql fix dataset dropduplicates',\n",
              " 'spark write structuredstreaming wal stream instead materializing',\n",
              " 'spark ml mllib optimize naivebayes mllib wrapper eliminate extra pass data',\n",
              " 'spark ml mllib optimize naivebayes mllib wrapper eliminate extra pass data',\n",
              " 'spark ml pyspark update nb python api add weight col parameter',\n",
              " 'spark sql self evident window function frame boundary api',\n",
              " 'spark streaming kafka alternative eliminate race condition poll twice',\n",
              " 'spark core expand blacklist tasksets',\n",
              " 'spark core expand blacklist tasksets',\n",
              " 'spark core add flag ignore corrupt files',\n",
              " 'build closing stale prs',\n",
              " 'spark docs add pointers wiki contributing md readme md warnings pull request template',\n",
              " 'spark sparkr support parallelizing r data frame larger gb',\n",
              " 'spark sql resolve null pointer exception casting empty string interval type',\n",
              " 'spark sql reject invalid join methods join columns specified pyspark dataframe join',\n",
              " 'spark mllib optimize kmeans implementation remove runs',\n",
              " 'spark streaming kafka doc make clear reusing group id bad',\n",
              " 'spark doc url linking accumulatorv document incorrect',\n",
              " 'fix hadoop version building spark md',\n",
              " 'spark sql introduce static sql conf',\n",
              " 'spark pyspark creating sparkcontext python without spark submit ignores user conf',\n",
              " 'spark ml sparkr fix sparkr spark naivebayes error label numeric type',\n",
              " 'spark pyspark pyspark rdd repartitioning results highly skewed partition sizes',\n",
              " 'spark sql tests fix flaky topic deletion kafkasourcestresssuite',\n",
              " 'spark sql support expression canonicalize',\n",
              " 'spark sql mark data type apis stable developerapi',\n",
              " 'spark sql follow add global temp view',\n",
              " 'spark pyspark upgraded version pyrolite',\n",
              " 'spark ml rformula supports forcing index label',\n",
              " 'spark ml rformula supports forcing index label',\n",
              " 'spark simplify dataframe api defining frame boundaries window functions',\n",
              " 'spark simplify dataframe api defining frame boundaries window functions',\n",
              " 'spark simplify dataframe api defining frame boundaries window functions',\n",
              " 'spark spark sql unify tie options single place jdbc datasource package',\n",
              " 'spark spark sql unify tie options single place jdbc datasource package',\n",
              " 'spark core fix concurrentmodificationexception issue blockstatusesaccumulator',\n",
              " 'spark sql handle duplicated field names header consistently null empty strings csv',\n",
              " 'spark test fix flaky test columntypesuite',\n",
              " 'spark ml remove superfluous split continuous features decision tree training',\n",
              " 'spark mesos enable gpu support mesos',\n",
              " 'spark docs remove unused generate changelist py',\n",
              " 'spark annotate spark sql package interfacestability',\n",
              " 'spark core fix partitions reliable rdd checkpointing',\n",
              " 'hot fix sql tests remove unused function sparksqlparsersuite',\n",
              " 'spark sql add global temp view',\n",
              " 'spark sql grammar parse top level nested data fields separately',\n",
              " 'spark sql tableidentifier quotedstring creates un parseable names name contains backtick',\n",
              " 'minor sql use resource path test script sh',\n",
              " 'spark core small sum count mean evaluator problems suboptimalities',\n",
              " 'spark web ui sorting description job stage page always work',\n",
              " 'minor ml remove redundant comment logisticregression',\n",
              " 'hotfix build use contains option jdbcrelationprovider',\n",
              " 'spark sql fix bug join key rewritten hashjoin',\n",
              " 'spark sql remove mutablerow',\n",
              " 'spark sql support spilling python udf',\n",
              " 'spark sparkr support options mode read write apis options types',\n",
              " 'spark sql streaming add textfile structured streaming',\n",
              " 'spark sql followup clean jdbcrelationprovider',\n",
              " 'spark webui web ui prevents spark submit application finished',\n",
              " 'spark introduce interfacestability annotation',\n",
              " 'spark stop reporting spill metrics shuffle metrics',\n",
              " 'spark stop reporting spill metrics shuffle metrics',\n",
              " 'spark sql deprecate approxcountdistinct todegrees toradians according functionregistry',\n",
              " 'spark sql deprecate approxcountdistinct todegrees toradians according functionregistry',\n",
              " 'spark web ui sorting stage job tables reload page table',\n",
              " 'spark streaming build add kafka project build modules',\n",
              " 'spark pyspark fix sqlcontext read text pass list paths',\n",
              " 'spark ml l bfgs solver linear regression accept general numeric label column types',\n",
              " 'spark tests upgrade docker client dependency',\n",
              " 'spark sql report throwable user streamexecution',\n",
              " 'spark sql remove redundant experimental annotations sql streaming',\n",
              " 'spark sql fix create view interval arithmetic',\n",
              " 'build closing stale prs',\n",
              " 'minor ml avoid array flatten nb training',\n",
              " 'spark sql test maven generate sql test jar fix maven build',\n",
              " 'spark sql add kafka source structured streaming',\n",
              " 'spark sql last returns wrong result case empty partition',\n",
              " 'spark tests mock sparkcontext reduce memory usage blockmanagersuite',\n",
              " 'spark ml doc update user guide multiclass logistic regression',\n",
              " 'spark sql fix npe explain describe table',\n",
              " 'spark sql parse scientific decimal literals decimals',\n",
              " 'spark sparkr read df write df api taking path optionally sparkr',\n",
              " 'spark sql add hash capability semantically equivalent hive',\n",
              " 'spark sql collect table size stat driver cached relation',\n",
              " 'sparkr doc minor formatting output cleanup r vignettes',\n",
              " 'spark ml parity check ml mllib test suites nb',\n",
              " 'spark core sql fix misaligned record accesses sparc architectures',\n",
              " 'spark webui spark history server summary page slow even set spark history ui maxapplications',\n",
              " 'spark mllib persist edges storage level non periodicgraphcheckpointer',\n",
              " 'spark input output add voidobjectinspector',\n",
              " 'spark sql code generation including many mutable states exceeds jvm size limit',\n",
              " 'spark sql select null via jdbc triggers illegalargumentexception thriftserver',\n",
              " 'spark sql allow complex expression input value based case statement',\n",
              " 'spark python mllib sparsevector getitem follow getitem contract',\n",
              " 'spark pyspark remove unnecessary py j listconverter patch',\n",
              " 'spark docs mllib make loss function formulation label note clearer mllib docs',\n",
              " 'spark sql generate column level statistics',\n",
              " 'spark documentation sparkr update r readme rmarkdown',\n",
              " 'spark documentation sparkr update r readme rmarkdown',\n",
              " 'spark sql web ui user friendly name spark thrift server web ui',\n",
              " 'spark core sql skip fix test cases windows due limitation windows',\n",
              " 'spark sql wrapping catalyst datatype hive data type avoid',\n",
              " 'spark ml mllib chisqselector performance improvement',\n",
              " 'spark sql add exist find methods catalog follow',\n",
              " 'spark spark tests mock interpose hdfs ensure streams closed',\n",
              " 'spark spark tests mock interpose hdfs ensure streams closed',\n",
              " 'minor doc add date description default serialization shuffling',\n",
              " 'spark sql collapse adjacent similar window operators',\n",
              " 'spark core making peer selection block replication pluggable',\n",
              " 'spark sql add unnamed version addreferenceobj minor objects',\n",
              " 'spark sql fix array map columnar cache',\n",
              " 'spark ml follow revert change nb model load maintain compatibility model stored',\n",
              " 'spark ml refactor naivebayes support weighted instances',\n",
              " 'spark sql add exist find methods catalog',\n",
              " 'spark ml fixed bug summary calculations pattern match label without casting',\n",
              " 'spark doc test run root admin user',\n",
              " 'spark doc test run root admin user',\n",
              " 'spark core fshistoryprovider ignore hidden files',\n",
              " 'spark mllib ml fix multiplying transposed sparsematrix sparsevector',\n",
              " 'spark sql support describe table partition sql syntax',\n",
              " 'spark sql remove unnecessary distincts multiple unions',\n",
              " 'spark support parsing json string columns',\n",
              " 'spark scheduler make task launch logs debug',\n",
              " 'spark spark history server web ui takes long single application',\n",
              " 'spark spark history server web ui takes long single application',\n",
              " 'spark core taskscheduler really needs offers indexedseq',\n",
              " 'docs reorganize explanation accumulators broadcast variables',\n",
              " 'minor docs fix th doc spark streaming kinesis',\n",
              " 'spark sql sparksession read jdbc use sql syntax cassandra support',\n",
              " 'spark ml mllib chisqselector performance improvement',\n",
              " 'spark follow ml enforce ml test exception local distributed dataset',\n",
              " 'spark sql fix invalid pushdown data independent filters beneath aggregates',\n",
              " 'spark hotfix fix classcircularityerror replsuite tests maven build use class forname instead utils classforname',\n",
              " 'spark hotfix fix classcircularityerror replsuite tests maven build use class forname instead utils classforname',\n",
              " 'spark sql collect list collect set collect null values',\n",
              " 'spark sql move row datasource related tests jdbcsuite',\n",
              " 'spark sql incorrect exchange reuse rowdatasourcescan',\n",
              " 'spark core add failedstages abortstage fetch failure',\n",
              " 'minor pyspark docs fix examples pyspark documentation',\n",
              " 'spark ml mllib ml doc updated ml mllib feature selection docs chisqselector',\n",
              " 'spark sparkr followup check null first layers spark mlp avoid warnings test results',\n",
              " 'spark ensure recordreaders closed data source file scans',\n",
              " 'spark core fix wrong assert regarding unroll memory memorystore',\n",
              " 'spark guard invalid comparisons unsaferow formats',\n",
              " 'spark sql break windowexec scala multiple files',\n",
              " 'spark sql desc formatted view lacks view definition',\n",
              " 'spark sql mark children final unary binary leaf expressions plan nodes',\n",
              " 'spark sql support pushing filters decimal timestamp types orc',\n",
              " 'spark sql use deprecated listtype api parquetschemaconverter',\n",
              " 'spark set spark caller context hdfs yarn',\n",
              " 'spark ml mlib add python api multinomial logistic regression',\n",
              " 'spark sql introduce implementation dense format unsafearraydata',\n",
              " 'fix two comments since actor used anymore',\n",
              " 'spark follow sparkr sparkr spark addfile supports adding directory recursively',\n",
              " 'docs update spark standalone md fix link',\n",
              " 'spark fix confusing exception message reserving capacity',\n",
              " 'spark sql read partition data reading new files filestream without globbing',\n",
              " 'spark core log many spark events got dropped livelistenerbus',\n",
              " 'spark ml add testimplicits ml unit tests promote todf',\n",
              " 'spark sql make dataframewrite save work jdbc',\n",
              " 'spark follow ml refactor chisqselector add ml python api',\n",
              " 'spark malformed url throw exceptions bricking executors',\n",
              " 'spark sql add dataframe api null ordering',\n",
              " 'minor sparkr add sparkr vignettes html gitignore',\n",
              " 'spark ml probabilisticclassifiermodels thresholds one',\n",
              " 'spark ml word vec accept non null string array addition existing null string array',\n",
              " 'spark sparkr set r package version number along mvn',\n",
              " 'spark add cpu time metrics',\n",
              " 'spark remove comparable requirement offset',\n",
              " 'spark sparkr sparkr zip distributed executors running sparkr rstudio',\n",
              " 'spark sparkr ml mllib make default params sparkr spark mlp consistent multilayerperceptronclassifier',\n",
              " 'spark pyspark core refactor pyspark accumulator api top accumulator v',\n",
              " 'build closes stale prs',\n",
              " 'spark sql avoid using default batchid filestreamsource fileentry',\n",
              " 'spark ml random forests communicate fewer trees iteration',\n",
              " 'spark build add jce jar buildclasspath building',\n",
              " 'spark sql remove hardcode agg plan hashaggregateexec',\n",
              " 'spark spark test make unit test added work',\n",
              " 'spark ml ml persistence backward compatibility lda',\n",
              " 'spark ml ml persistence backward compatibility lda',\n",
              " 'spark sql support single distinct aggregate combined non partial aggregate',\n",
              " 'spark streaming stop jvm streamingcontext python process dead',\n",
              " 'spark base paths end return empty dataframes',\n",
              " 'skip building r vignettes spark built',\n",
              " 'spark core remove kill multiple executors together reduce rpc call time',\n",
              " 'sql minor correct comment sortbasedaggregationiterator safeproj',\n",
              " 'spark ml mllib add treeaggregatedepth parameter aftsurvivalregression',\n",
              " 'spark docs documenting current treatment maven opts',\n",
              " 'spark sql set expectedoutputattributes converting simplecatalogrelation logicalrelation',\n",
              " 'spark sql fix reading cataloged data sources without extending schemarelationprovider',\n",
              " 'spark sql override sameresult hivetablescanexec make reuseexchange work text format table',\n",
              " 'spark sql sessioncatalog tableexists check temp view',\n",
              " 'spark sql changeprecision compact decimal respect rounding mode',\n",
              " 'spark mark streaming providers experimental',\n",
              " 'spark follow sparkr ml fix print kolmogorov smirnov test summary',\n",
              " 'spark sparkr core sparkr support add files spark job get executors',\n",
              " 'spark make structuredstreaming filestreamsource batch generation faster',\n",
              " 'spark core avoid formatting python path yarn mesos cluster mode',\n",
              " 'spark core clarify type taskendreason failed task',\n",
              " 'spark core allow driver advertise different network address',\n",
              " 'spark ml better error wls cases like singular input',\n",
              " 'spark prevent kinesis asl assembly artifacts published',\n",
              " 'spark sql analyze cte definitions allow cte subquery define cte',\n",
              " 'core minor add minor code change taskstate task',\n",
              " 'spark sql remove uesless rowseparator variable set auto expanding buffer default maxcharspercolumn option csv',\n",
              " 'spark sql remove uesless rowseparator variable set auto expanding buffer default maxcharspercolumn option csv',\n",
              " 'spark ml add nan value handling bucketizer',\n",
              " 'spark mllib ml add chisquare selector based false positive rate fpr test',\n",
              " 'spark prevent listingfilecatalog failing path exist',\n",
              " 'spark sql remainder expression eval returns incorrect result double value',\n",
              " 'spark mllib use bounded priority queue find synonyms word vecmodel',\n",
              " 'spark pyspark core pyspark sparkcontext addfile supports adding files recursively',\n",
              " 'core doc fix errors comments',\n",
              " 'spark sql streaming follw fix filestream source sink log get configuration issue',\n",
              " 'minor build fix checkstyle error',\n",
              " 'spark sql make streamexecution garbage collect metadata',\n",
              " 'spark sql make streamexecution garbage collect metadata',\n",
              " 'spark yarn test make shuffle service test really test auth',\n",
              " 'spark sql revert collect table size stat driver cached relation',\n",
              " 'spark sql streaming add ability remove old metadatalog filestreamsource',\n",
              " 'spark sql use hadoopconf insertintohivetable',\n",
              " 'spark sql fix multiple bugs ddl statements temporary views',\n",
              " 'spark add uiweburl javasparkcontext pyspark sparkcontext',\n",
              " 'revert spark sql make streamexecution garbage collect metadata',\n",
              " 'spark sql make streamexecution garbage collect metadata',\n",
              " 'spark sql make streamexecution garbage collect metadata',\n",
              " 'spark ml unified logisticregression interface',\n",
              " 'spark ml unified logisticregression interface',\n",
              " 'spark ml unified logisticregression interface',\n",
              " 'spark properly escape field names code generated error messages',\n",
              " 'spark sql fix python udf filter top outer join',\n",
              " 'spark sql bring back separator sql ui',\n",
              " 'spark webui show application executorlimit application page',\n",
              " 'spark sql fixing docker integration tests error due different versions jars',\n",
              " 'spark docs clarify window slide duration absolute time relative calendar',\n",
              " 'spark sql assertonquery condition always return boolean value',\n",
              " 'spark spark spark sql make csv cast null values properly',\n",
              " 'spark spark spark sql make csv cast null values properly',\n",
              " 'spark spark spark sql make csv cast null values properly',\n",
              " 'spark spark spark sql make csv cast null values properly',\n",
              " 'spark build call static member via instance reference',\n",
              " 'spark deploy start scripts use hostname f',\n",
              " 'spark sql improve check double values equality rule',\n",
              " 'spark sql fix ddl bugs table management name temp view exists',\n",
              " 'spark sql block users specify internal data source provider hive',\n",
              " 'spark close serialization stream fix wrong answer bug putiteratorasbytes',\n",
              " 'spark sql followup fix instances calls list length size n',\n",
              " 'spark docs remove extra table tags configuration document',\n",
              " 'spark core implement bitset clearuntil use merge joins',\n",
              " 'spark mllib word vecmodel findsynonyms longer spuriously rejects best match invoked vector',\n",
              " 'spark docs use valid url spark rdd paper',\n",
              " 'correct fetchsize property name docs',\n",
              " 'spark sql collect table size stat driver cached relation',\n",
              " 'spark docs dataframewriter documentation formatting problems',\n",
              " 'spark bump hadoop version',\n",
              " 'spark sql refactor treenode tojson avoid oom converting unknown fields json',\n",
              " 'spark sql refactor treenode tojson avoid oom converting unknown fields json',\n",
              " 'spark tests increase timeouts directkafkastreamsuite tests',\n",
              " 'spark missing log j config file tests common network',\n",
              " 'spark sql alias specified aggregates pivot honored',\n",
              " 'spark prevent invalid block locations reported put exceptions',\n",
              " 'spark sql antlr lexer wrongly treats full qualified identifier decimal number token parsing sql string',\n",
              " 'spark sql use implicitcastinputtypes function length',\n",
              " 'spark sql fix aggregates grouped literals empty input',\n",
              " 'spark ensure temp shuffle data file cleaned error',\n",
              " 'spark ensure temp shuffle data file cleaned error',\n",
              " 'spark build upgrade netty final bug fixes',\n",
              " 'spark core coarsegrainedexecutorbackend inform driver self kill',\n",
              " 'spark build hotfix mima excludes fix',\n",
              " 'spark sql minor performance improvement jdbc batch inserts',\n",
              " 'spark web ui limit timeline executor events',\n",
              " 'spark error use sparkcontext makerdd seq',\n",
              " 'spark tests use specified spark buffer pagesize',\n",
              " 'spark ml mllib check weight vector size ann',\n",
              " 'spark spark fixed multiple bugs alter table',\n",
              " 'spark spark fixed multiple bugs alter table',\n",
              " 'spark spark core inappropriate memory management org apache spark storage memorystore may lead memory leak',\n",
              " 'spark pyspark better error message serialization failures large objects python',\n",
              " 'spark core make collectionaccumulator setaccumulator value read thread safely',\n",
              " 'spark yarn dynamic allocation avoid marking released container failed',\n",
              " 'spark sql support nulls first last clause order',\n",
              " 'minor sql add missing functions options sqlconf use applicable',\n",
              " 'spark df take df limit collect perform python',\n",
              " 'spark df take df limit collect perform python',\n",
              " 'spark sql optimize query ctas',\n",
              " 'spark docs reference asf page main place find third party packages',\n",
              " 'spark sql improve performance removing caching list length n',\n",
              " 'core doc remove redundant comment',\n",
              " 'spark python remove sparkcontext clearfiles pyspark api removed scala api prior spark',\n",
              " 'spark documentation relation heartbeatinterval',\n",
              " 'spark sparkr add sparkr vignette',\n",
              " 'spark sql add statistics describe formatted',\n",
              " 'spark initialize hive listeners execution client',\n",
              " 'spark sql complex query triggers binding error hashaggregateexec',\n",
              " 'spark collectlimit execute perform per partition limits',\n",
              " 'build closing stale prs ones suggested closed committer',\n",
              " 'spark sql fix python udf takeorderedandprojectexec',\n",
              " 'spark prevent failed remote reads cached blocks failing entire job',\n",
              " 'spark post mima exclusion build changes',\n",
              " 'spark post mima exclusion build changes',\n",
              " 'spark post mima exclusion build changes',\n",
              " 'spark refactoring blockmanager status reporting block removal',\n",
              " 'spark core fix memory leak memory store unable cache whole rdd memory',\n",
              " 'spark core minor fix default partitioner cannot partition array keys error message pairrddfunctions',\n",
              " 'spark pyspark use map comprehension doc',\n",
              " 'spark performance improvement partitioner defaultpartitioner without sortby',\n",
              " 'spark web ui dag list partitions graph',\n",
              " 'spark remove unused taskmetricsuidata updatedblockstatuses field',\n",
              " 'spark sql better error message driver side broadcast join ooms',\n",
              " 'spark follow ml change kmeans k means default init steps',\n",
              " 'spark pyspark fix appending multiple times pythonpath spark config sh',\n",
              " 'spark spark ut clean spark warehouse ut',\n",
              " 'spark spark ut clean spark warehouse ut',\n",
              " 'spark sql fixing compression issues approximate quantiles adding tests',\n",
              " 'spark ml mllib kmeans speedup better choice k means init steps',\n",
              " 'spark mllib sparkr fix return description sparkr mlp summary method',\n",
              " 'spark core share task support unionrdd instances',\n",
              " 'spark follow ml sparkr r mllib algorithms support input columns features label',\n",
              " 'spark graphx parallel implementation personalized pagerank',\n",
              " 'spark sql filesourcescanexec extract outputordering information',\n",
              " 'spark sql partitioning dates timestamps work parquet vectorized reader',\n",
              " 'spark yarnshuffleservice handle moving credentials leveldb',\n",
              " 'streaming doc correction',\n",
              " 'spark sparkr ml sparkr spark als argument reg default',\n",
              " 'spark core utility parsing spark versions',\n",
              " 'spark web ui spark master ui reverse proxy application workers ui',\n",
              " 'spark rowbasedkeyvaluebatch use default page size prevent ooms',\n",
              " 'spark project infra build sparkr automate building testing windows currently sparkr',\n",
              " 'spark sparkr additional arguments write df passed data source',\n",
              " 'spark sql preprocessddl respect case sensitivity checking duplicated columns',\n",
              " 'spark sql remove duplicate test cases auto join hivecompatibilitysuite scala',\n",
              " 'spark shuffle service files invalidated slave lost',\n",
              " 'minor sql fixing typo unit test',\n",
              " 'spark sql function size return parameter null',\n",
              " 'spark sparkr core fix r tests use path touri sparkcontext windows paths sparkr',\n",
              " 'spark sql mllib use arraybuffer instead arraybuffer append performance critical paths',\n",
              " 'spark r dapply return array raw columns',\n",
              " 'spark sql streaming avoid serialization issues using arrays save file names filestreamsource',\n",
              " 'spark sql simplify logic converting data source table hive compatible format',\n",
              " 'spark test flaky test org apache spark sql hive statisticssuite',\n",
              " 'spark resubmitted shuffle outputs get deleted zombie map tasks',\n",
              " 'spark core fix ask type parameter removeexecutor',\n",
              " 'spark yarn clean logging yarn',\n",
              " 'spark sql simplify parser join processing',\n",
              " 'spark fix streamcorruptionexception blockmanager getremotevalues',\n",
              " 'spark fix streamcorruptionexception blockmanager getremotevalues',\n",
              " 'minor remove unnecessary check mlserde',\n",
              " 'spark trim ltrim rtrim strips characters spaces',\n",
              " 'spark build upgrade snappy java',\n",
              " 'spark spark sql make address values portable longtounsaferowmap',\n",
              " 'spark spark sql make address values portable longtounsaferowmap',\n",
              " 'spark sql better error messages parsing json using dataframereader',\n",
              " 'minor ml correct weights doc multilayerperceptronclassificationmodel',\n",
              " 'spark sql fix memory issue generating json treenode',\n",
              " 'spark sql file based external table without path created',\n",
              " 'spark sql cached table parquet orc shard beelines',\n",
              " 'spark sql metastorerelation tojson throws assertexception due missing othercopyargs',\n",
              " 'spark sql better error message exceptions scalaudf execution',\n",
              " 'spark sql support table level statistics generation storing loading metastore',\n",
              " 'spark sql allow specify database table view name rename',\n",
              " 'spark sql error handling ctas data source table using overwrite mode',\n",
              " 'minor ml mllib remove work around breeze sparse matrix',\n",
              " 'spark mllib standardize python java mllib api accept optional long seeds cases',\n",
              " 'spark mllib standardize python java mllib api accept optional long seeds cases',\n",
              " 'spark improved spark core code replacing pattern match boolean value else block',\n",
              " 'spark sql remove direct usage hiveclient insertintohivetable',\n",
              " 'spark sparkr sparkr sc setloglevel work',\n",
              " 'spark sparkr kolmogorov smirnov test sparkr wrapper',\n",
              " 'spark sql fix arraytype maptype catalogstring',\n",
              " 'minor sql dropping necessary tables',\n",
              " 'spark sql examples encoder dataset example incorrect type',\n",
              " 'spark ml mllib fix multivariantonlinesummerizer numnonzeros',\n",
              " 'sparkr minor fix docs sparkr session count',\n",
              " 'spark sql require explicit cross join cartesian products',\n",
              " 'spark reusing dictionary column decoding consecutive row groups throw error',\n",
              " 'spark sql pass optimized query queryexecution dataframewriter',\n",
              " 'spark sparkr followup change since version',\n",
              " 'spark yarnshuffleservice init properly yarn rolling upgrade',\n",
              " 'sparkr doc regexp extract doc returns empty string match fails',\n",
              " 'spark sparkr spark version available r',\n",
              " 'spark pyspark using hivecontext creating sparkcontext spark throws java lang illegalstateexception cannot call methods stopped sparkcontext',\n",
              " 'spark refactor jdbcrdd expose resultset seq row utility methods',\n",
              " 'spark sql try whole dataset immediately first partition',\n",
              " 'spark sql verification function related externalcatalog apis',\n",
              " 'spark webui executor computing time negative number calculation error',\n",
              " 'sql doc minor add scala specific java specific',\n",
              " 'spark ml sparkr r mllib algorithms support input columns features label',\n",
              " 'spark sparkr sql decimal type properly cast number collecting sparkdataframe',\n",
              " 'spark webui style event timeline broken',\n",
              " 'spark webui style event timeline broken',\n",
              " 'spark sql add unit test compare table partition column metadata',\n",
              " 'spark sql set right number partitions reading data local collection',\n",
              " 'spark add shuffle service metrics entry monitoring docs',\n",
              " 'spark add shuffle service metrics entry monitoring docs',\n",
              " 'spark sql enable row based hashmap hashaggregateexec',\n",
              " 'spark workaround hive hiveresultsetmetadata issigned exception',\n",
              " 'spark sql support partition batch pruning predicate inmemorytablescanexec',\n",
              " 'spark sql remove unused codes subexpressioneliminationforwholestagecodegen',\n",
              " 'spark sql remove partition columns partition metadata',\n",
              " 'spark sql remove partition columns partition metadata',\n",
              " 'spark hotfix fix compilation scala',\n",
              " 'spark core mllib avoid allocating length arrays',\n",
              " 'spark sql add hexadecimal literal parsing',\n",
              " 'spark core resolve deadlocking driver executors die',\n",
              " 'spark sql remove redundant semanticequals sortorder',\n",
              " 'spark sql physical plan create table ctas take catalogtable',\n",
              " 'spark spark spark sql fix multiple bugs create table like command',\n",
              " 'spark spark spark sql fix multiple bugs create table like command',\n",
              " 'spark spark spark sql fix multiple bugs create table like command',\n",
              " 'fixed typos',\n",
              " 'spark sql implements percentile approx aggregation function supports partial aggregation',\n",
              " 'spark build build prs pyarn unless yarn code changed',\n",
              " 'spark tests fix replsuite replicating blocks object class defined repl',\n",
              " 'revert pr pr',\n",
              " 'spark sparkr mllib sparkr spark glm configurable regularization parameter',\n",
              " 'sparkr minor fix windowpartitionby example',\n",
              " 'spark sparkr fix jvm api tests sparkr',\n",
              " 'spark tests fix mesoscoarsegrainedschedulerbackendsuite',\n",
              " 'spark sparkr fix tests hivecontext sparkr skipped always',\n",
              " 'spark core make java loggers static members',\n",
              " 'spark core make coarsegrainedschedulerbackend removeexecutor non blocking',\n",
              " 'spark add build profile flags entry mesos build module',\n",
              " 'minor sparkr verbose build comment windows md rather promoting default build without hive',\n",
              " 'spark spark spark sql create alterviewascommand handle alter view',\n",
              " 'spark spark spark sql create alterviewascommand handle alter view',\n",
              " 'spark spark spark sql create alterviewascommand handle alter view',\n",
              " 'spark sparkr sparksubmit allow set sparkr shell command conf',\n",
              " 'spark sql eliminate redundant cast array without null map without null',\n",
              " 'spark tests fix replsuite replicating blocks object class defined repl',\n",
              " 'spark web ui spark history server load large application history',\n",
              " 'spark core use netty defaultthreadfactory enable fast threadlocal impl',\n",
              " 'spark fix perf issue caused tasksetmanager abortifcompletelyblacklisted',\n",
              " 'spark core add encrypted shuffle spark',\n",
              " 'spark core add encrypted shuffle spark',\n",
              " 'minor mllib sql clean unused variables unused import',\n",
              " 'minor docs fix minor typos python example code',\n",
              " 'spark sql datastreamwriter document supports parquet',\n",
              " 'spark core test stop env params output jenkins job page',\n",
              " 'spark sql table existence checking index table name exists',\n",
              " 'spark sql fix bug satisfy sort requirements partial aggregations',\n",
              " 'spark added spark warehouse dev rat excludes',\n",
              " 'spark sql remove unused classtag field atomictype base class',\n",
              " 'spark sparkr make jvm backend calling functions public',\n",
              " 'spark sql improve performance msck repair table hive metastore',\n",
              " 'sparkr minor fix lda doc',\n",
              " 'fixed typo',\n",
              " 'build closes stale prs',\n",
              " 'spark sql planner adds un necessary sort even child ordering semantically required ordering',\n",
              " 'spark ml enable standardscaler standardize sparse vectors withmean true',\n",
              " 'spark ui fix event timeline bars length',\n",
              " 'ml mllib require condition message match sparsematrix',\n",
              " 'spark sql fix bug sampling replacement',\n",
              " 'spark sql move join optimizer rules separate file',\n",
              " 'spark sql move expression optimizer rules separate file',\n",
              " 'spark sql move subquery optimizer rules file',\n",
              " 'spark sql move finish analysis optimization stage file',\n",
              " 'spark sql move object optimization rules file',\n",
              " 'spark test add empty strings regressiontests prefixcomparatorssuite',\n",
              " 'spark catalyst pushdown non deterministic join conditions',\n",
              " 'spark sql support purging old logs metadatalog',\n",
              " 'spark sql add bigdecimal literal',\n",
              " 'spark move mesos module',\n",
              " 'spark mllib fix comparing vector bug testingutils',\n",
              " 'spark sql filestreamsource track list seen files indefinitely',\n",
              " 'spark sql remove hiveclient setcurrentdatabase hivesessioncatalog',\n",
              " 'spark sql issue exception users specify partitioning columns without given schema',\n",
              " 'sparkr minor fix example spark naivebayes',\n",
              " 'spark sql follow improve document typedimperativeaggregate',\n",
              " 'spark minor move createtables hivestrategies',\n",
              " 'spark sql followup enable timestamp type tests json verify unsupported types csv',\n",
              " 'spark document update links external dstream projects',\n",
              " 'spark sql typecoercion supports widening conversion datetype timestamptype',\n",
              " 'spark sql supports using arbitrary java object internal aggregation buffer object',\n",
              " 'spark core make sparkconf serializable',\n",
              " 'spark literal sql handle infinity nan',\n",
              " 'spark sql postgresdialect widen float short types reads',\n",
              " 'sparkr build ignore cran check r folder',\n",
              " 'spark core avoid building debug trace log messages unless respective log level enabled',\n",
              " 'spark spark spark sql fix outer join elimination filter isnotnull constraints unable filter null supplying rows',\n",
              " 'spark spark spark sql fix outer join elimination filter isnotnull constraints unable filter null supplying rows',\n",
              " 'spark spark spark sql fix outer join elimination filter isnotnull constraints unable filter null supplying rows',\n",
              " 'spark sql skip unnecessary final group input data already clustered group keys',\n",
              " 'spark ml pyspark pyspark lir lor supports tree aggregation level configurable',\n",
              " 'spark ml pyspark pyspark lir lor supports tree aggregation level configurable',\n",
              " 'spark spark sql mapobjects make copies unsafe backed data',\n",
              " 'spark spark sql mapobjects make copies unsafe backed data',\n",
              " 'spark core hadooprdd npe debug log level getlocationinfo null',\n",
              " 'spark sql method sqlcontext parsedatatype datatypestring string could removed',\n",
              " 'spark sql removal hivesharedstate',\n",
              " 'spark sql infer propagate non deterministic constraints',\n",
              " 'sparkr minor add installation message remote master mode improve messages',\n",
              " 'sparkr minor add examples window function docs',\n",
              " 'minor sparkr fix r mllib parameter documentation',\n",
              " 'spark sql read write timestamps dates iso dateformat timestampformat option csv json',\n",
              " 'spark web ui history server oom due unlimited taskuidata',\n",
              " 'spark sql add prettyname row number dense rank percent rank cume dist',\n",
              " 'spark pyspark java launched pyspark gateway may java used spark environment',\n",
              " 'spark mllib sparkr multilayer perceptron classifier wrapper sparkr',\n",
              " 'sparkr minor fix doc show method',\n",
              " 'minor doc fix wrong ml feature normalizer document',\n",
              " 'spark ml fix invalidargumentexception issue quantilediscretizer quantiles duplicated',\n",
              " 'minor build fix java checkstyle error',\n",
              " 'spark sql remove catalog table type index',\n",
              " 'minor sql remove implemented functions comments hivesessioncatalog scala',\n",
              " 'spark configurable buffer size unsafesorterspillreader',\n",
              " 'spark use single quotes generating sql string literals',\n",
              " 'trivial typo fix',\n",
              " 'minor doc use standard quotes instead curly quote marks mac structured streaming programming guides',\n",
              " 'sparkr minor remove reference link common windows environment variables',\n",
              " 'spark sql add next expression sqlexception cause',\n",
              " 'spark documentation latex scala doc play nicely',\n",
              " 'spark documentation latex scala doc play nicely',\n",
              " 'spark use catalystconf resolver case sensitivity comparison',\n",
              " 'spark sql moves class quantilesummaries project catalyst implementing percentile approx',\n",
              " 'spark sql moves class quantilesummaries project catalyst implementing percentile approx',\n",
              " 'sparkr minor update r description file',\n",
              " 'spark sql mark collect non deterministic',\n",
              " 'spark sparkr add cran documentation checks run tests sh',\n",
              " 'spark follow ml add expert param support sharedparamscodegen',\n",
              " 'spark sql removal useless createhivetableasselectlogicalplan',\n",
              " 'spark spark core certain classes fail deserialize block manager replication',\n",
              " 'spark spark core certain classes fail deserialize block manager replication',\n",
              " 'spark sparkr doc updates cran check fixes',\n",
              " 'spark range support sql generation',\n",
              " 'minor sql fix typos comments test hints',\n",
              " 'sparkr minor add xiangrui felix maintainers',\n",
              " 'spark sparkr r mllib refactor cleanup reformat fix deprecation test',\n",
              " 'spark doc document g heap region effect spark vs',\n",
              " 'sparkr minor fix cache folder path windows',\n",
              " 'spark pyspark ml add missing num features num classes',\n",
              " 'spark streaming documentation actual code differs unsupported operations',\n",
              " 'spark sql decrease threshold split expressions',\n",
              " 'spark document additional options jdbc writer',\n",
              " 'spark make unaligned access unsafe available aarch',\n",
              " 'spark sql move hive hack data source table hiveexternalcatalog',\n",
              " 'spark sql fix nullpropagation optimizer handle count null correctly',\n",
              " 'minor r add sparkr rcheck sparkr tar gz r gitignore',\n",
              " 'spark core document spark ssl protocol required ssl',\n",
              " 'spark follow sparkr robust test case spark gaussianmixture',\n",
              " 'spark ml make tree aggregation level linear logistic regression configurable',\n",
              " 'spark core sparksubmit packages fix default conf exist dependent module',\n",
              " 'spark sql relationalgroupeddataset agg preserve order allow multiple aggregates per column',\n",
              " 'spark sql logicalrelation newinstance follow semantics multiinstancerelation',\n",
              " 'sparkr example change example app name',\n",
              " 'spark sparkr fix cran undocumented duplicated arguments warnings',\n",
              " 'spark pyspark ml improve handling pyspark pipeline used without stages',\n",
              " 'spark sql support sql generation inline tables',\n",
              " 'spark sql change error message range numeric literals',\n",
              " 'spark sql array sql testing array related functions',\n",
              " 'spark sparkr alternating least squares als wrapper',\n",
              " 'spark shuffle job failure due executor oom offheap mode',\n",
              " 'spark core unknownhostexception thrown namenode ha enabled',\n",
              " 'spark web ui new executor page removed conditional logs thread dump columns',\n",
              " 'spark sql whitelist operators predicate pushdown',\n",
              " 'spark mllib pyspark fix bound checking sparsevector',\n",
              " 'spark ml minmaxscaler remain nan value',\n",
              " 'spark core fixed one error biased randomizeinplace',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'spark ml add multiclass logistic regression spark ml',\n",
              " 'hotfix compilation broken due protected ctor',\n",
              " 'spark sql support type coercion foldable expression inline tables',\n",
              " 'spark ml sparkr lda wrapper sparkr',\n",
              " 'spark sql null fail analysis',\n",
              " 'spark expose spark range table valued function sql',\n",
              " 'spark mesos documentation making spark mesos honor port restrictions',\n",
              " 'spark mesos documentation making spark mesos honor port restrictions',\n",
              " 'spark sql remove redundant pushdown rule union',\n",
              " 'spark sql support partial aggregation reducegroups',\n",
              " 'spark sql minor code cleanup unresolvedordinal',\n",
              " 'spark sql treenodeexception flat mapping relationalgroupeddataset created dataframe containing column created lit expr',\n",
              " 'spark streaming fix metrics retrieval source lastreceivedbatch',\n",
              " 'spark sql streaming improve exception string reported streamingquerylistener',\n",
              " 'spark core sql purge superfluous fs calls',\n",
              " 'spark sparkr ml gaussian mixture model wrapper sparkr',\n",
              " 'spark yarn fix couple races cluster app initialization',\n",
              " 'spark sql bypass userdefinedgenerator json format check',\n",
              " 'spark sql simplify subqueryexpression interface',\n",
              " 'spark sql generated specificsafeprojection apply method grows beyond kb',\n",
              " 'spark sparkr isotonic regression wrapper sparkr',\n",
              " 'spark sql add test cases methods parserutils',\n",
              " 'minor doc fix descriptions properties argument documenation jdbc apis',\n",
              " 'spark sql make view usage visible analysis',\n",
              " 'spark sql rename parserutils assert validate',\n",
              " 'spark docs remove api doc link mapreducetriplets operator',\n",
              " 'spark sparkr handle sparkr rdd generics create warnings r cmd check',\n",
              " 'minor sparkr spark glm weightcol signature',\n",
              " 'spark sql pyspark improve timestamp lose precision cases',\n",
              " 'minor doc correct code snippet results quick start documentation',\n",
              " 'spark sql remove private hive sql hive execution package',\n",
              " 'spark sql adds expression unresolvedordinal represent ordinals group order',\n",
              " 'spark sql serde storage properties limitations',\n",
              " 'spark sql improve error message encountering incompatible datasourceregister',\n",
              " 'spark pyspark sql create dataframe dict row schema',\n",
              " 'spark core sql consolidate code variable substitution',\n",
              " 'spark sparkr split docs arrange orderby methods',\n",
              " 'spark ml mllib update logisticcostaggregator serialization code make consistent linearregression',\n",
              " 'trivial ml fix logisticregression typo error message',\n",
              " 'spark mesos make spark mesos honor port restrictions coarse grain mode',\n",
              " 'wip minor typo fix several trivival typos',\n",
              " 'spark ml avoid integer overflow polynomialexpansion getpolysize',\n",
              " 'spark sql core app name randomuuid even spark app name exists',\n",
              " 'spark build upgrade kafka release',\n",
              " 'spark add additional options jdbc creating new table',\n",
              " 'minor core fix warnings depreciated methods mesosclusterschedulersuite diskblockobjectwritersuite',\n",
              " 'spark documentation documentation link examples',\n",
              " 'doc add config option spark ui enabled document',\n",
              " 'spark sql clause fall infinite loop',\n",
              " 'spark ml mllib gaussianmixture use treeaggregate improve performance',\n",
              " 'spark sql test added test case verifying table identifier parsing',\n",
              " 'minor doc fix style examples across documentation',\n",
              " 'spark change dataformat yyyymmddhhmm yyyymmddhhmmss',\n",
              " 'spark sql parse negative numeric literals',\n",
              " 'spark sql column partition path starting handled correctly',\n",
              " 'minor ml rename treeensemblemodels treeensemblemodel pyspark',\n",
              " 'spark sql avoid per record type dispatch json reading',\n",
              " 'spark pyspark spark submit allow set pythonexec driver executor conf',\n",
              " 'spark yarn handle potential deadlock driver handling messages',\n",
              " 'spark web ui fix executor dead alive executor ui',\n",
              " 'spark core add shutdown hook driverrunner prevent driver process leak',\n",
              " 'spark sql literals sql testing literal parsing',\n",
              " 'spark sql simplify constructor parameters quantilesummaries',\n",
              " 'spark sql reuse subqueries within query',\n",
              " 'spark lookup spark home directory executor uri set',\n",
              " 'spark examples doc fix examples consistent indentation documentation',\n",
              " 'spark use concurrenthashmap instead scala map sparksqloperationmanager',\n",
              " 'correct example value spark ssl yyy xxx settings',\n",
              " 'spark sql group order ordinal arithmetic tests',\n",
              " 'spark sql support testing exceptions sqlquerytestsuite',\n",
              " 'spark minor doc wrong description memory management document',\n",
              " 'spark sql move test data files test data folder',\n",
              " 'spark spark sql normalization isolation sqlquerytestsuite',\n",
              " 'spark spark sql normalization isolation sqlquerytestsuite',\n",
              " 'spark yarn add configurable credential manager spark running yarn',\n",
              " 'spark yarn add configurable credential manager spark running yarn',\n",
              " 'spark core honor spark ui retainedstages reduce mem pressure',\n",
              " 'spark sql recursive call columnvector getint breaks jit inlining',\n",
              " 'spark sparkr add install spark function',\n",
              " 'spark sparkr ml spark glm support weightcol',\n",
              " 'spark spark sql push filter rowgroups level parquet reader',\n",
              " 'spark spark sql push filter rowgroups level parquet reader',\n",
              " 'spark sql fix construction file path hadoop path',\n",
              " 'spark sql infrastructure file based sql end end tests',\n",
              " 'spark sql regexp extract doc returns empty string match fails',\n",
              " 'spark spark override task properties dispatcher',\n",
              " 'spark spark override task properties dispatcher',\n",
              " 'typo fow',\n",
              " 'spark sql better error messages creating table select without enabling hive support',\n",
              " 'spark sql support minus set operator',\n",
              " 'spark sql rebuild table comment retrieving metadata hive metastore',\n",
              " 'minor sparkr r api documentation coltypes confusing',\n",
              " 'fixed typo',\n",
              " 'make logdir easily copy paste able',\n",
              " 'spark make applicationstate max num retry configurable',\n",
              " 'spark sql ddl msck repair table',\n",
              " 'spark pyspark fromoffsets parameter support kafkautils createdirectstream python',\n",
              " 'spark ml fix aftaggregator aftsurvivalregression serializes unnecessary data',\n",
              " 'spark sql remove private sql private spark sql execution package',\n",
              " 'spark enable history server links dispatcher ui',\n",
              " 'spark sql checkanswer raise testfailedexception wrong results',\n",
              " 'spark mesos spark application throws exception exit',\n",
              " 'spark core misleading warning sparkcontext getorcreate warn sparkcontext use existing sparkcontext configuration may take effect',\n",
              " 'spark sql add orc compress alias compression option',\n",
              " 'spark sql fix spark sql thrift server fetchresults bug',\n",
              " 'spark sql adds argument type information typed logical plan like mapelements typedfilter appendcolumn',\n",
              " 'spark sql simplify processing logic lead lag processing',\n",
              " 'update docs include sasl support rpc',\n",
              " 'spark trivial avoid using postfix operators add much remove whitelisting',\n",
              " 'spark make requesttotalexecutors public developer api consistent requestexecutors killexecutors',\n",
              " 'spark core handle jvm errors printed stdout',\n",
              " 'spark sql case sensitivity support refresh temp table',\n",
              " 'spark sql fix wrong messages ctas partition clause',\n",
              " 'spark sql adds auxiliary info like input class input schema typedaggregateexpression',\n",
              " 'spark sql correlated subqueries containing non deterministic operations return incorrect results',\n",
              " 'spark fix java lint errors',\n",
              " 'spark ml leastsquaresaggregators serializes unnecessary data',\n",
              " 'spark configurable update interval console progress bar',\n",
              " 'spark sql fix build error using tuple explicitly stringfunctionssuite',\n",
              " 'spark sql regexp extract optional groups causes npe',\n",
              " 'spark spark core streaming postgresql jdbc driver',\n",
              " 'spark fix links programming guide',\n",
              " 'spark docs summary add spark sql broadcasttimeout docs sql programming gu',\n",
              " 'spark docs changed programming guide reference old accumulator api scala',\n",
              " 'spark docs changed programming guide reference old accumulator api scala',\n",
              " 'document mesos cluster mode supports python',\n",
              " 'spark master call schedule executor exit events failures',\n",
              " 'spark python docs fix api doc references udfregistration update important classes',\n",
              " 'spark web ui mask spark authenticate secret spark environ',\n",
              " 'spark sql prevent potentially read corrupt statstics binary parquet vectorized reader',\n",
              " 'spark hive settings hive site xml may overridden hive default values',\n",
              " 'spark follow ml add transformschema stringindexer vectorassembler fix failed tests',\n",
              " 'spark core add ganglia dmax parameter',\n",
              " 'spark examples ml improve ml example outputs',\n",
              " 'spark examples ml improve ml example outputs',\n",
              " 'spark sql switch java net uri parse url',\n",
              " 'spark sql general data types mapped oracle',\n",
              " 'minor update accumulatorv doc mention',\n",
              " 'spark streaming kafka doc doc kafka integration',\n",
              " 'spark sql unify logical plans create table ctas',\n",
              " 'spark sql make datasetbenchmark fairer among dataset dataframe rdd',\n",
              " 'spark sql fix performance regression parquet table vectorized parquet record reader used',\n",
              " 'maintenance cleaning stale prs',\n",
              " 'hotfix remove unnecessary imports broke build',\n",
              " 'spark shuffle cache shuffle index file speedup shuffle fetch',\n",
              " 'spark ml probabilisticclassifier fit check threshoulds length',\n",
              " 'spark build add rules preventing use java annotations deprecated override',\n",
              " 'spark ml mllib make ann training data persisted needed',\n",
              " 'spark sql add args checking dataset randomsplit sample',\n",
              " 'spark move datasourcescanexec existingrdd scala file',\n",
              " 'spark sql fix overflow longtounsaferowmap',\n",
              " 'spark sql fix overflow longtounsaferowmap',\n",
              " 'spark sql fixes encoder error dataset typed select',\n",
              " 'spark sql createtable altertable externalcatalog take db',\n",
              " 'spark sql implements eval method expression assertnotnull',\n",
              " 'minor sql fix minor formatting issue sortaggregateexec tostring',\n",
              " 'spark core fix spillreader npe spillfile data',\n",
              " 'spark sql fix deprecated parquet constructor usage',\n",
              " 'spark build fix jline dependency management version sca',\n",
              " 'spark sql register driverclass rather user specified class',\n",
              " 'spark sql refactor datasourcescanexec partition discovery execution instead planning time',\n",
              " 'spark spark spark array map greatest least type coercion handle decimal type',\n",
              " 'spark spark spark array map greatest least type coercion handle decimal type',\n",
              " 'spark spark spark array map greatest least type coercion handle decimal type',\n",
              " 'spark python fixed bug crossvalidator avgmetrics',\n",
              " 'sql minor use stricter type parameter make clear parquet reader returns unsaferow',\n",
              " 'spark web ui visible passwords spark environment page',\n",
              " 'spark sql test removal testhivesharedstate',\n",
              " 'spark sparkcontext addfile throw called twice file',\n",
              " 'spark sql move greatest least conditionalexpressions scala arithmetic scala',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LSE4AWloJNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data_features, df[\"severity\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2o7T2DantGv",
        "colab_type": "code",
        "outputId": "6e05c42b-c454-4af8-98c2-58955b31df4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "print (\"Training the random forest...\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize a Random Forest classifier with 100 trees\n",
        "forest = RandomForestClassifier(n_estimators = 100) \n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as\n",
        "# features and the sentiment labels as the response variable\n",
        "forest.fit(X_train, y_train)\n",
        "f1_score(y_test, forest.predict(X_test))\n",
        "# # This may take a few minutes to run\n",
        "# score = cross_val_score(forest, train_data_features, df[\"severity\"], cv=5).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the random forest...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-ddb12ba68c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# features and the sentiment labels as the response variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# # This may take a few minutes to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# score = cross_val_score(forest, train_data_features, df[\"severity\"], cv=5).mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1253\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te0hzGi3nwAB",
        "colab_type": "code",
        "outputId": "a14c0f6f-0311-41a5-8997-1d29da949d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "\n",
        "Percept = Perceptron()\n",
        "rand_forest = RandomForestClassifier()\n",
        "SGDC = SGDClassifier()\n",
        "mas_of_result = list()\n",
        "zoo = [\n",
        "    (Percept, 'Perceptron'),\n",
        "    (rand_forest, 'RandomForestClassifier'), \n",
        "    (SGDC, 'SGDClassifier'),\n",
        "    ]\n",
        "for model, name in zoo:\n",
        "    start_time = time.time()\n",
        "    mas_of_result.append([name, cross_val_score(model, train_data_features, df[\"severity\"], cv=3).mean()])\n",
        "    print(f\"{name}      {time.time() - start_time}s seconds ---\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perceptron      50.28874850273132s seconds ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a35a3be98511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzoo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmas_of_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"severity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name}      {time.time() - start_time}s seconds ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 231\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCCyw9bmmJdv",
        "colab_type": "code",
        "outputId": "425c4dde-3817-4c7b-e021-114be87fe9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for name, accuracy in mas_of_result:\n",
        "    print(f'{name} has {round(accuracy,4)*100}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perceptron has 39.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUUN2pDfKKyp",
        "colab_type": "code",
        "outputId": "f0248d82-b4a2-4c38-e487-efa441370cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "model = word2vec.Word2Vec(clean_train_message, size=300, workers=-1)\n",
        "#создадим словарь со словами и соответсвующими им векторами\n",
        "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0707 12:01:45.786561 140571391973248 word2vec.py:1546] Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "W0707 12:01:46.411437 140571391973248 base_any2vec.py:1357] EPOCH - 1 : supplied example count (0) did not equal expected count (17966)\n",
            "W0707 12:01:46.437421 140571391973248 base_any2vec.py:1362] EPOCH - 1 : supplied raw word count (0) did not equal expected count (5363150)\n",
            "W0707 12:01:46.448242 140571391973248 base_any2vec.py:1357] EPOCH - 2 : supplied example count (0) did not equal expected count (17966)\n",
            "W0707 12:01:46.466101 140571391973248 base_any2vec.py:1362] EPOCH - 2 : supplied raw word count (0) did not equal expected count (5363150)\n",
            "W0707 12:01:46.484288 140571391973248 base_any2vec.py:1357] EPOCH - 3 : supplied example count (0) did not equal expected count (17966)\n",
            "W0707 12:01:46.501851 140571391973248 base_any2vec.py:1362] EPOCH - 3 : supplied raw word count (0) did not equal expected count (5363150)\n",
            "W0707 12:01:46.509474 140571391973248 base_any2vec.py:1357] EPOCH - 4 : supplied example count (0) did not equal expected count (17966)\n",
            "W0707 12:01:46.532565 140571391973248 base_any2vec.py:1362] EPOCH - 4 : supplied raw word count (0) did not equal expected count (5363150)\n",
            "W0707 12:01:46.540240 140571391973248 base_any2vec.py:1357] EPOCH - 5 : supplied example count (0) did not equal expected count (17966)\n",
            "W0707 12:01:46.546863 140571391973248 base_any2vec.py:1362] EPOCH - 5 : supplied raw word count (0) did not equal expected count (5363150)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDsGXFTlKKvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "class tfidf_vectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = len(next(iter(w2v.values())))\n",
        "\n",
        "    def fit(self, X):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CjovRmkKKtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_mean = tfidf_vectorizer(w2v).fit(clean_train_message).transform(clean_train_message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNiM41skL1v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci2pcRBPa2gr",
        "colab_type": "code",
        "outputId": "3d6b3d0e-01b9-44fc-bee8-54aadac8dfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "results_percept = list()\n",
        "Percept = Perceptron()\n",
        "results_percept.append(cross_val_score(model, data_mean, df[\"severity\"].map({0:0, 1:1, 2:1, 3:1, 4:1}), cv=3).mean())\n",
        "Percept = Perceptron()\n",
        "results_percept.append(cross_val_score(model, data_mean, df[\"severity\"].map({0:1, 1:0, 2:1, 3:1, 4:1}), cv=3).mean())\n",
        "Percept = Perceptron()\n",
        "results_percept.append(cross_val_score(model, data_mean, df[\"severity\"].map({0:1, 1:1, 2:0, 3:1, 4:1}), cv=3).mean())\n",
        "Percept = Perceptron()\n",
        "results_percept.append(cross_val_score(model, data_mean, df[\"severity\"].map({0:1, 1:1, 2:1, 3:0, 4:1}), cv=3).mean())\n",
        "Percept = Perceptron()\n",
        "results_percept.append(cross_val_score(model, data_mean, df[\"severity\"].map({0:1, 1:1, 2:1, 3:1, 4:0}), cv=3).mean())\n",
        "for i, score in enumerate(results_percept):\n",
        "  print(f'for {i} score is {score*100}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for 0 score is 83.96367882794401%\n",
            "for 1 score is 60.12448172350713%\n",
            "for 2 score is 51.65869019350736%\n",
            "for 3 score is 80.90787984012402%\n",
            "for 4 score is 84.92110377970091%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYJT-cMobbi4",
        "colab_type": "code",
        "outputId": "9278a2ba-4638-4a27-859b-2832070f1a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "severities = {0,1,2,3,4}\n",
        "list1 = severities.copy()\n",
        "severities.remove(1)\n",
        "print(severities)\n",
        "print(list1)\n",
        "df[\"severity\"].map({0:0, (1,2,3,4):1})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 2, 3, 4}\n",
            "{0, 1, 2, 3, 4}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-175-a0c2efb86227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseverities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"severity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: tuple() takes at most 1 argument (4 given)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O34z141iA-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = {(0, 2.0): 'кортежи могут быть ключами',\n",
        "              1: 'целые числа могут быть ключами',\n",
        "              'бежать': 'строки тоже', }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFTQ2_75iIfv",
        "colab_type": "code",
        "outputId": "274d6f8b-2830-41c8-f734-0379e6db13f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "dictionary[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-cd6842156ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs8V4k5wKKqN",
        "colab_type": "code",
        "outputId": "3cad715f-a85d-4e4b-dae8-60fe625d8662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "  Percept = Perceptron()\n",
        "rand_forest = RandomForestClassifier()\n",
        "SGDC = SGDClassifier()\n",
        "log_reg = LogisticRegression()\n",
        "adaboost = AdaBoostClassifier()\n",
        "tree = DecisionTreeClassifier()\n",
        "mas_of_result = list()\n",
        "zoo = [\n",
        "    (Percept, 'Perceptron')\n",
        "    (rand_forest, 'RandomForestClassifier'), \n",
        "    (SGDC, 'SGDClassifier'),\n",
        "    (Percept, 'Perceptron'),\n",
        "    (log_reg, 'LogisticRegression'),\n",
        "    (adaboost, 'AdaBoostClassifier'), \n",
        "    (tree, 'DecisionTreeClassifier'),\n",
        "    ]\n",
        "for model, name in zoo:\n",
        "    start_time = time.time()\n",
        "    mas_of_result.append([name, cross_val_score(model, data_mean, df[\"severity\"], cv=3).mean()])\n",
        "    print(f\"{name}      {time.time() - start_time}s seconds ---\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier      10.11588740348816s seconds ---\n",
            "SGDClassifier      4.576531171798706s seconds ---\n",
            "Perceptron      1.7284836769104004s seconds ---\n",
            "LogisticRegression      5.765891790390015s seconds ---\n",
            "AdaBoostClassifier      78.73874473571777s seconds ---\n",
            "DecisionTreeClassifier      28.80396866798401s seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0GPMwkmURyu",
        "colab_type": "code",
        "outputId": "e4a67561-beb0-4e3f-ce9b-268c8cbb483b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "for name, score in mas_of_result:\n",
        "  print(f'{name} has {score*100}% accuracy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier has 50.82921078609232% accuracy\n",
            "SGDClassifier has 57.82032702350212% accuracy\n",
            "Perceptron has 57.82032702350212% accuracy\n",
            "LogisticRegression has 57.82032702350212% accuracy\n",
            "AdaBoostClassifier has 57.519733256981674% accuracy\n",
            "DecisionTreeClassifier has 38.08288921584473% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoBgaOJgKKn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_mean, df[\"severity\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoiQFburKKln",
        "colab_type": "code",
        "outputId": "a47de250-2491-4ebc-bb1c-021d00e1b0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "Percept = Perceptron()\n",
        "rand_forest = RandomForestClassifier()\n",
        "SGDC = SGDClassifier()\n",
        "log_reg = LogisticRegression()\n",
        "adaboost = AdaBoostClassifier()\n",
        "tree = DecisionTreeClassifier()\n",
        "svm_svc = SVC()\n",
        "zoo = [\n",
        "    (rand_forest, 'RandomForestClassifier'), \n",
        "    (SGDC, 'SGDClassifier'),\n",
        "    (Percept, 'Perceptron'),\n",
        "    (log_reg, 'LogisticRegression'),\n",
        "    (adaboost, 'AdaBoostClassifier'), \n",
        "    (tree, 'DecisionTreeClassifier'),\n",
        "    (svm_svc, 'SVC')\n",
        "  ]\n",
        "for model, name in zoo:\n",
        "  start_time = time.time()\n",
        "  model.fit(X_train, y_train)\n",
        "  print(f\"{name}      {time.time() - start_time}s seconds --- with {accuracy_score(y_test, model.predict(X_test))*100}% accuracy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier      3.666513681411743s seconds --- with 49.73285841495993% accuracy\n",
            "SGDClassifier      1.44639253616333s seconds --- with 57.34639358860196% accuracy\n",
            "Perceptron      0.6362392902374268s seconds --- with 24.888691006233305% accuracy\n",
            "LogisticRegression      1.8689754009246826s seconds --- with 57.34639358860196% accuracy\n",
            "AdaBoostClassifier      29.206193685531616s seconds --- with 57.235084594835264% accuracy\n",
            "DecisionTreeClassifier      10.291683673858643s seconds --- with 38.557435440783614% accuracy\n",
            "SVC      94.58903884887695s seconds --- with 57.34639358860196% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5pSMXXnKKi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sugsHxFOKKge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33WBHFCNKKd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdHnOfQcKKbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t7uK8SeKKZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQf8gX7HKKXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZAw2BQKKUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bf8xDhSKKQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEjm7iI4mJbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHypVVxYmJPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkMHdlR7mJMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enOpngQRiQ1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QmOFCwgiQzL",
        "colab_type": "code",
        "outputId": "1169f253-bf02-48df-f9c7-069e9bd049b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UEkMS3YiQwF",
        "colab_type": "code",
        "outputId": "82b78948-895c-4458-b3ff-ddc501f9b329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text.split('\\n')[3:-5])\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "df['Processed_message'] = df.message_encoding.apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHAtv-LiQrv",
        "colab_type": "code",
        "outputId": "5076ef39-3431-4117-c98e-670b7019cef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.Processed_message.apply(lambda x: len(x.split(\" \"))).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.42240899476789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MghVrURiQpW",
        "colab_type": "code",
        "outputId": "dea0ec3a-a11b-4855-af5a-b41ea649339c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Convolution1D\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "max_features = 6000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(df['Processed_message'])\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_message'])\n",
        "\n",
        "maxlen = 32\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "y = df['severity']\n",
        "\n",
        "embed_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_size))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(5, activation=\"softmax\"))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 3\n",
        "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12576 samples, validate on 5390 samples\n",
            "Epoch 1/3\n",
            "12576/12576 [==============================] - 22s 2ms/step - loss: 1.2780 - acc: 0.5619 - val_loss: 1.1812 - val_acc: 0.5818\n",
            "Epoch 2/3\n",
            "12576/12576 [==============================] - 12s 973us/step - loss: 1.1730 - acc: 0.5667 - val_loss: 1.1783 - val_acc: 0.5818\n",
            "Epoch 3/3\n",
            "12576/12576 [==============================] - 8s 668us/step - loss: 1.1630 - acc: 0.5768 - val_loss: 1.1772 - val_acc: 0.5818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8f12b8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9qufshTiQnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFypZenNiQk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fCZBfziiQie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}